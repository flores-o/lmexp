{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install -e ../../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lmexp.models.implementations.gpt2small import GPT2Tokenizer, SteerableGPT2\n",
    "from lmexp.generic.direction_extraction.probing import train_probe, load_probe\n",
    "from lmexp.generic.direction_extraction.caa import get_caa_vecs\n",
    "from lmexp.generic.get_locations import at_search_tokens, all_tokens\n",
    "from lmexp.generic.activation_steering.steering_approaches import (\n",
    "    add_multiplier,\n",
    ")\n",
    "from lmexp.generic.activation_steering.steerable_model import SteeringConfig\n",
    "from datetime import datetime\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model and tokenizer\n",
    "\n",
    "These classes have already implemented all the probing-related methods so we won't have to add more hooks + they are ready to use with our vector extraction and steering functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SteerableGPT2()\n",
    "tokenizer = GPT2Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.n_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a linear probe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate some data\n",
    "\n",
    "Let's see whether we can get a date/time probe vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_labeled_text(n):\n",
    "    # date as text, date as utc timestamp in seconds, sample randomly from between 1990 and 2022\n",
    "    start_timestamp = datetime(2013, 1, 1).timestamp()\n",
    "    end_timestamp = datetime(2016, 1, 1).timestamp()\n",
    "    labeled_text = []\n",
    "    for i in range(n):\n",
    "        timestamp = start_timestamp + (end_timestamp - start_timestamp) * random.random()\n",
    "        date = datetime.fromtimestamp(timestamp)\n",
    "        # date like \"Monday 15th November 2021 8AM\"\n",
    "        text = date.strftime(\"Today is a %A. It's the %dth of %B, %Y. The time is %I %p. This is the point in time when\")\n",
    "        label = timestamp\n",
    "        labeled_text.append((text, label))\n",
    "    # normalize labels to have mean 0 and std 1\n",
    "    labels = [label for _, label in labeled_text]\n",
    "    mean = sum(labels) / len(labels)\n",
    "    std = (sum((label - mean) ** 2 for label in labels) / len(labels)) ** 0.5\n",
    "    labeled_text = [(text, (label - mean) / std) for text, label in labeled_text]\n",
    "    return labeled_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"Today is a Monday. It's the 18th of November, 2013. The time is 10 PM. This is the point in time when\", -0.7128079377320918)\n"
     ]
    }
   ],
   "source": [
    "data = gen_labeled_text(10_000)\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We train a probe with activations extracted from the ' when' token\n"
     ]
    }
   ],
   "source": [
    "# We train a probe with activations extracted from the \"when\" token\n",
    "search_tokens = tokenizer.encode(\"This is the point in time when\")[0][-1:]\n",
    "print(\n",
    "    f\"We train a probe with activations extracted from the '{tokenizer.decode(search_tokens)}' token\"\n",
    ")\n",
    "save_to = \"gpt2small_date_probe.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 78/78 [01:28<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, mean loss: 5.2001126152038575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [01:28<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, mean loss: 0.9435745620727539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [01:29<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, mean loss: 0.8537364158630371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [01:28<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, mean loss: 0.7758437637329102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [01:28<00:00,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, mean loss: 0.7150826072692871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "probe = train_probe(\n",
    "    labeled_text=data,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    layer=4,\n",
    "    n_epochs=5,\n",
    "    batch_size=128,\n",
    "    lr=1e-2,\n",
    "    token_location_fn=at_search_tokens,\n",
    "    search_tokens=search_tokens,\n",
    "    save_to=save_to,\n",
    "    loss_type=\"mse\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe = load_probe(save_to).to(model.device)\n",
    "direction = probe.weight[0]\n",
    "bias = probe.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.0031], requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'input': 'The current date is', 'output': 'The current date is the date of the first occurrence'}]\n"
     ]
    }
   ],
   "source": [
    "results = model.generate_with_steering(\n",
    "    text=[\"The current date is\"],\n",
    "    tokenizer=tokenizer,\n",
    "    steering_configs=[\n",
    "        SteeringConfig(\n",
    "            layer=4,\n",
    "            vector=direction.detach(),\n",
    "            scale=-5,\n",
    "            steering_fn=add_multiplier,\n",
    "            token_location_fn=all_tokens,\n",
    "        )\n",
    "    ],\n",
    "    max_n_tokens=10,\n",
    "    save_to=None,\n",
    ")\n",
    "print(results['results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'input': 'The current date is', 'output': 'The current date is the next day, so it'}]\n"
     ]
    }
   ],
   "source": [
    "results = model.generate_with_steering(\n",
    "    text=[\"The current date is\"],\n",
    "    tokenizer=tokenizer,\n",
    "    steering_configs=[\n",
    "        SteeringConfig(\n",
    "            layer=4,\n",
    "            vector=direction.detach(),\n",
    "            scale=10,\n",
    "            steering_fn=add_multiplier,\n",
    "            token_location_fn=all_tokens,\n",
    "        )\n",
    "    ],\n",
    "    max_n_tokens=10,\n",
    "    save_to=None,\n",
    ")\n",
    "print(results[\"results\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get some contrast pairs\n",
    "\n",
    "Let's try an easy direction - positive vs negative sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOD = [\n",
    "    \"The weather is really nice\",\n",
    "    \"I'm so happy\",\n",
    "    \"This cake is absolutely delicious\",\n",
    "    \"I love my friends\",\n",
    "    \"I'm feeling great\",\n",
    "    \"I'm so excited\",\n",
    "    \"This is the best day ever\",\n",
    "    \"I really like this gift\",\n",
    "    \"Croissants are my favorite\",\n",
    "    \"The movie was fantastic\",\n",
    "    \"I got a promotion at work\",\n",
    "    \"My vacation was amazing\",\n",
    "    \"The concert exceeded my expectations\",\n",
    "    \"I'm grateful for my family\",\n",
    "    \"This book is incredibly engaging\",\n",
    "    \"The restaurant service was excellent\",\n",
    "    \"I'm proud of my accomplishments\",\n",
    "    \"The sunset is breathtakingly beautiful\",\n",
    "    \"I passed my exam with flying colors\",\n",
    "    \"This coffee tastes perfect\",\n",
    "]\n",
    "\n",
    "BAD = [\n",
    "    \"The weather is really bad\",\n",
    "    \"I'm so sad\",\n",
    "    \"This cake is completely inedible\",\n",
    "    \"I hate my enemies\",\n",
    "    \"I'm feeling awful\",\n",
    "    \"I'm so anxious\",\n",
    "    \"This is the worst day ever\",\n",
    "    \"I dislike this gift\",\n",
    "    \"Croissants are disgusting\",\n",
    "    \"The movie was terrible\",\n",
    "    \"I got fired from work\",\n",
    "    \"My vacation was a disaster\",\n",
    "    \"The concert was a huge disappointment\",\n",
    "    \"I'm frustrated with my family\",\n",
    "    \"This book is incredibly boring\",\n",
    "    \"The restaurant service was horrible\",\n",
    "    \"I'm ashamed of my mistakes\",\n",
    "    \"The weather is depressingly gloomy\",\n",
    "    \"I failed my exam miserably\",\n",
    "    \"This coffee tastes awful\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    (text, True) for text in GOOD\n",
    "] + [\n",
    "    (text, False) for text in BAD\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the CAA vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 21.75it/s]\n"
     ]
    }
   ],
   "source": [
    "vectors = get_caa_vecs(\n",
    "    labeled_text=dataset,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    layers=range(3, 8),\n",
    "    token_location_fn=all_tokens,\n",
    "    save_to=None,\n",
    "    batch_size=6              \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the CAA vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'input': 'I think that this cat is', 'output': \"I think that this cat is a bit of a liability. I think that it's a liability that\"}]\n"
     ]
    }
   ],
   "source": [
    "results = model.generate_with_steering(\n",
    "    text=[\"I think that this cat is\"],\n",
    "    tokenizer=tokenizer,\n",
    "    steering_configs=[\n",
    "        SteeringConfig(\n",
    "            layer=5,\n",
    "            vector=vectors[5],\n",
    "            scale=-1,\n",
    "            steering_fn=add_multiplier,\n",
    "            token_location_fn=all_tokens,\n",
    "        ),\n",
    "        SteeringConfig(\n",
    "            layer=4,\n",
    "            vector=vectors[4],\n",
    "            scale=-1,\n",
    "            steering_fn=add_multiplier,\n",
    "            token_location_fn=all_tokens,\n",
    "        ),\n",
    "    ],\n",
    "    max_n_tokens=20,\n",
    "    save_to=None,\n",
    ")\n",
    "print(results[\"results\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'input': 'I think that this cat is', 'output': 'I think that this cat is a great example of how to use the cat as a companion.\\n'}]\n"
     ]
    }
   ],
   "source": [
    "results = model.generate_with_steering(\n",
    "    text=[\"I think that this cat is\"],\n",
    "    tokenizer=tokenizer,\n",
    "    steering_configs=[\n",
    "        SteeringConfig(\n",
    "            layer=5,\n",
    "            vector=vectors[5],\n",
    "            scale=1,\n",
    "            steering_fn=add_multiplier,\n",
    "            token_location_fn=all_tokens,\n",
    "        ),\n",
    "        SteeringConfig(\n",
    "            layer=4,\n",
    "            vector=vectors[4],\n",
    "            scale=1,\n",
    "            steering_fn=add_multiplier,\n",
    "            token_location_fn=all_tokens,\n",
    "        ),\n",
    "    ],\n",
    "    max_n_tokens=20,\n",
    "    save_to=None,\n",
    ")\n",
    "print(results[\"results\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
