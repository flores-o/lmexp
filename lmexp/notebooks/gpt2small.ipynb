{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lmexp.models.implementations.gpt2small import GPT2Tokenizer, SteerableGPT2\n",
    "from lmexp.generic.direction_extraction.probing import train_probe, load_probe\n",
    "from lmexp.generic.direction_extraction.caa import get_caa_vecs\n",
    "from lmexp.generic.get_locations import from_search_tokens, all_tokens\n",
    "from lmexp.generic.activation_steering.steering_approaches import (\n",
    "    add_multiplier,\n",
    ")\n",
    "from lmexp.generic.activation_steering.steerable_model import SteeringConfig\n",
    "from datetime import datetime\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model and tokenizer\n",
    "\n",
    "These classes have already implemented all the probing-related methods so we won't have to add more hooks + they are ready to use with our vector extraction and steering functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SteerableGPT2()\n",
    "tokenizer = GPT2Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, device(type='cpu'))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.n_layers, model.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a linear probe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate some data\n",
    "\n",
    "Let's see whether we can get a date/time probe vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_labeled_text(n):\n",
    "    # date as text, date as utc timestamp in seconds, sample randomly from between 1990 and 2022\n",
    "    start_timestamp = datetime(2013, 1, 1).timestamp()\n",
    "    end_timestamp = datetime(2016, 1, 1).timestamp()\n",
    "    labeled_text = []\n",
    "    for _ in range(n):\n",
    "        timestamp = start_timestamp + (end_timestamp - start_timestamp) * random.random()\n",
    "        date = datetime.fromtimestamp(timestamp)\n",
    "        text = tokenizer.chat_format(\n",
    "            [\n",
    "                {\"role\": \"user_1\", \"content\": \"What is the date\"},\n",
    "                {\"role\": \"user_2\", \"content\": date.strftime(\"Today's date is %B %d, %Y\")},\n",
    "            ]\n",
    "        )\n",
    "        label = timestamp\n",
    "        labeled_text.append((text, label))\n",
    "    # normalize labels to have mean 0 and std 1\n",
    "    labels = [label for _, label in labeled_text]\n",
    "    mean = sum(labels) / len(labels)\n",
    "    std = (sum((label - mean) ** 2 for label in labels) / len(labels)) ** 0.5\n",
    "    labeled_text = [(text, (label - mean) / std) for text, label in labeled_text]\n",
    "    return labeled_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"user_1: What is the date\\nuser_2: Today's date is October 25, 2013\", -0.7869691126342264)\n"
     ]
    }
   ],
   "source": [
    "data = gen_labeled_text(10_000)\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We train a probe with activations extracted from the 'user_2' token\n"
     ]
    }
   ],
   "source": [
    "# We train a probe with activations extracted from the \"when\" token\n",
    "search_tokens = tokenizer.encode(\"\\nuser_2: Today is\")[0][1:4]\n",
    "print(\n",
    "    f\"We train a probe with activations extracted from the '{tokenizer.decode(search_tokens)}' token\"\n",
    ")\n",
    "save_to = \"gpt2small_date_probe.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:55<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, mean loss: 1.5892208450317382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:54<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, mean loss: 0.07932361764907837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:54<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, mean loss: 0.04544511251449585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:54<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, mean loss: 0.027392904090881347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:53<00:00,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, mean loss: 0.017819223898649216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(save_to):\n",
    "    probe = train_probe(\n",
    "        labeled_text=data,\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        layer=4,\n",
    "        n_epochs=5,\n",
    "        batch_size=128,\n",
    "        lr=1e-2,\n",
    "        token_location_fn=from_search_tokens,\n",
    "        search_tokens=search_tokens,\n",
    "        save_to=save_to,\n",
    "        loss_type=\"mse\",\n",
    "    )\n",
    "else:\n",
    "    probe = load_probe(save_to)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe = load_probe(save_to).to(model.device)\n",
    "direction = probe.weight[0]\n",
    "bias = probe.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.0274], requires_grad=True)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'input': \"user_1: What's the date?\\nuser_2: The date is\", 'output': \"user_1: What's the date?\\nuser_2: The date is the date of the first time you've ever been in a room\"}]\n"
     ]
    }
   ],
   "source": [
    "results = model.generate_with_steering(\n",
    "    text=[tokenizer.chat_format([{\"role\": \"user_1\", \"content\": \"What's the date?\"}, {\"role\": \"user_2\", \"content\": \"The date is\"}])],\n",
    "    tokenizer=tokenizer,\n",
    "    steering_configs=[\n",
    "        SteeringConfig(\n",
    "            layer=4,\n",
    "            vector=direction.detach(),\n",
    "            scale=-5,\n",
    "            steering_fn=add_multiplier,\n",
    "            token_location_fn=all_tokens,\n",
    "        )\n",
    "    ],\n",
    "    max_n_tokens=30,\n",
    "    save_to=None,\n",
    ")\n",
    "print(results['results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'input': \"user_1: What's the date?\\nuser_2: The date is\", 'output': \"user_1: What's the date?\\nuser_2: The date is the day of the week.\\nuser_3: The date\"}]\n"
     ]
    }
   ],
   "source": [
    "results = model.generate_with_steering(\n",
    "    text=[\n",
    "        tokenizer.chat_format(\n",
    "            [\n",
    "                {\"role\": \"user_1\", \"content\": \"What's the date?\"},\n",
    "                {\"role\": \"user_2\", \"content\": \"The date is\"},\n",
    "            ]\n",
    "        )\n",
    "    ],\n",
    "    tokenizer=tokenizer,\n",
    "    steering_configs=[\n",
    "        SteeringConfig(\n",
    "            layer=4,\n",
    "            vector=direction.detach(),\n",
    "            scale=5,\n",
    "            steering_fn=add_multiplier,\n",
    "            token_location_fn=all_tokens,\n",
    "        )\n",
    "    ],\n",
    "    max_n_tokens=30,\n",
    "    save_to=None,\n",
    ")\n",
    "print(results[\"results\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get some contrast pairs\n",
    "\n",
    "Let's try an easy direction - positive vs negative sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOD = [\n",
    "    \"The weather is really nice\",\n",
    "    \"I'm so happy\",\n",
    "    \"This cake is absolutely delicious\",\n",
    "    \"I love my friends\",\n",
    "    \"I'm feeling great\",\n",
    "    \"I'm so excited\",\n",
    "    \"This is the best day ever\",\n",
    "    \"I really like this gift\",\n",
    "    \"Croissants are my favorite\",\n",
    "    \"The movie was fantastic\",\n",
    "    \"I got a promotion at work\",\n",
    "    \"My vacation was amazing\",\n",
    "    \"The concert exceeded my expectations\",\n",
    "    \"I'm grateful for my family\",\n",
    "    \"This book is incredibly engaging\",\n",
    "    \"The restaurant service was excellent\",\n",
    "    \"I'm proud of my accomplishments\",\n",
    "    \"The sunset is breathtakingly beautiful\",\n",
    "    \"I passed my exam with flying colors\",\n",
    "    \"This coffee tastes perfect\",\n",
    "]\n",
    "\n",
    "BAD = [\n",
    "    \"The weather is really bad\",\n",
    "    \"I'm so sad\",\n",
    "    \"This cake is completely inedible\",\n",
    "    \"I hate my enemies\",\n",
    "    \"I'm feeling awful\",\n",
    "    \"I'm so anxious\",\n",
    "    \"This is the worst day ever\",\n",
    "    \"I dislike this gift\",\n",
    "    \"Croissants are disgusting\",\n",
    "    \"The movie was terrible\",\n",
    "    \"I got fired from work\",\n",
    "    \"My vacation was a disaster\",\n",
    "    \"The concert was a huge disappointment\",\n",
    "    \"I'm frustrated with my family\",\n",
    "    \"This book is incredibly boring\",\n",
    "    \"The restaurant service was horrible\",\n",
    "    \"I'm ashamed of my mistakes\",\n",
    "    \"The weather is depressingly gloomy\",\n",
    "    \"I failed my exam miserably\",\n",
    "    \"This coffee tastes awful\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    (text, True) for text in GOOD\n",
    "] + [\n",
    "    (text, False) for text in BAD\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the CAA vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 21.32it/s]\n"
     ]
    }
   ],
   "source": [
    "vectors = get_caa_vecs(\n",
    "    labeled_text=dataset,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    layers=range(3, 8),\n",
    "    token_location_fn=all_tokens,\n",
    "    save_to=None,\n",
    "    batch_size=6              \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the CAA vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'input': 'I think that this cat is', 'output': \"I think that this cat is a bit of a liability. I think that it's a liability that\"}]\n"
     ]
    }
   ],
   "source": [
    "results = model.generate_with_steering(\n",
    "    text=[\"I think that this cat is\"],\n",
    "    tokenizer=tokenizer,\n",
    "    steering_configs=[\n",
    "        SteeringConfig(\n",
    "            layer=5,\n",
    "            vector=vectors[5],\n",
    "            scale=-1,\n",
    "            steering_fn=add_multiplier,\n",
    "            token_location_fn=all_tokens,\n",
    "        ),\n",
    "        SteeringConfig(\n",
    "            layer=4,\n",
    "            vector=vectors[4],\n",
    "            scale=-1,\n",
    "            steering_fn=add_multiplier,\n",
    "            token_location_fn=all_tokens,\n",
    "        ),\n",
    "    ],\n",
    "    max_n_tokens=20,\n",
    "    save_to=None,\n",
    ")\n",
    "print(results[\"results\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'input': 'I think that this cat is', 'output': 'I think that this cat is a great example of how to use the cat as a companion.\\n'}]\n"
     ]
    }
   ],
   "source": [
    "results = model.generate_with_steering(\n",
    "    text=[\"I think that this cat is\"],\n",
    "    tokenizer=tokenizer,\n",
    "    steering_configs=[\n",
    "        SteeringConfig(\n",
    "            layer=5,\n",
    "            vector=vectors[5],\n",
    "            scale=1,\n",
    "            steering_fn=add_multiplier,\n",
    "            token_location_fn=all_tokens,\n",
    "        ),\n",
    "        SteeringConfig(\n",
    "            layer=4,\n",
    "            vector=vectors[4],\n",
    "            scale=1,\n",
    "            steering_fn=add_multiplier,\n",
    "            token_location_fn=all_tokens,\n",
    "        ),\n",
    "    ],\n",
    "    max_n_tokens=20,\n",
    "    save_to=None,\n",
    ")\n",
    "print(results[\"results\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
