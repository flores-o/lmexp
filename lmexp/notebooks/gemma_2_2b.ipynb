{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lmexp.models.implementations.gemma_2_2b import GemmaTokenizer, SteerableGemma\n",
    "from lmexp.models.constants import MODEL_GEMMA_2_2B\n",
    "from lmexp.models.model_helpers import (\n",
    "    input_to_prompt_gemma,\n",
    "    MODEL_ID_TO_END_OF_INSTRUCTION,\n",
    ")\n",
    "from lmexp.generic.direction_extraction.caa import get_caa_vecs\n",
    "from lmexp.generic.get_locations import after_search_tokens, all_tokens\n",
    "from lmexp.generic.activation_steering.steering_approaches import (\n",
    "    add_multiplier,\n",
    ")\n",
    "from lmexp.generic.activation_steering.steerable_model import SteeringConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemma-2-2b CAA Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92654ffd66f1489c96626638c48b03a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = SteerableGemma()\n",
    "tokenizer = GemmaTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, device(type='cuda', index=0))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.n_layers, model.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get some contrast pairs\n",
    "\n",
    "Let's try an easy direction - positive vs negative sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_answers = [\n",
    "    {\n",
    "        \"question\": \"How is the weather?\",\n",
    "        \"good_answer\": \"The weather is really nice\",\n",
    "        \"bad_answer\": \"The weather is really bad\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How are you feeling?\",\n",
    "        \"good_answer\": \"I'm so happy\",\n",
    "        \"bad_answer\": \"I'm so sad\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How is the cake?\",\n",
    "        \"good_answer\": \"This cake is absolutely delicious\",\n",
    "        \"bad_answer\": \"This cake is completely inedible\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How do you feel about your friends?\",\n",
    "        \"good_answer\": \"I love my friends\",\n",
    "        \"bad_answer\": \"I hate my friends\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How are you feeling today?\",\n",
    "        \"good_answer\": \"I'm feeling great\",\n",
    "        \"bad_answer\": \"I'm feeling awful\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [(input_to_prompt_gemma(example[\"question\"])+example[\"good_answer\"], True) for example in questions_answers]\n",
    "dataset += [\n",
    "    (input_to_prompt_gemma(example[\"question\"]) + example[\"bad_answer\"], False)\n",
    "    for example in questions_answers\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the CAA vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search tokens: tensor([107, 108])\n",
      "We will extract activations from after the '<end_of_turn>\n",
      "' token\n"
     ]
    }
   ],
   "source": [
    "search_tokens = tokenizer.encode(MODEL_ID_TO_END_OF_INSTRUCTION[MODEL_GEMMA_2_2B])[0, 1:]\n",
    "print(f\"Search tokens: {search_tokens}\")\n",
    "\n",
    "print(\n",
    "    f\"We will extract activations from after the '{tokenizer.decode(search_tokens)}' token\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  5.43it/s]\n"
     ]
    }
   ],
   "source": [
    "vectors = get_caa_vecs(\n",
    "    labeled_text=dataset,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    layers=range(0, 25),  # Adjust this range based on the number of layers in Gemma-2-2b\n",
    "    token_location_fn=after_search_tokens,\n",
    "    search_tokens=search_tokens,\n",
    "    save_to=None,\n",
    "    batch_size=6,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the CAA vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model output without steering:\n",
      "Answer: Yes.\n",
      "Answer: Yes.\n",
      "Answer: Yes.\n",
      "Answer: Yes.\n",
      "Answer: Yes.\n",
      "Answer: Yes.\n",
      "Answer: Yes.\n",
      "Answer\n",
      "\n",
      "Model output with positive steering:\n",
      "Answer: Yes, I love cats.\n",
      "Answer: Yes, I love cats.\n",
      "Answer: Yes, I love cats.\n",
      "Answer: Yes, I love cats.\n",
      "\n",
      "Model output with negative steering:\n",
      "Answer:\n",
      "I don't like cats.\n",
      "I don't like cats.\n",
      "I don't like cats.\n",
      "I don't like cats.\n",
      "I\n",
      "\n",
      "Summary:\n",
      "1. Without steering: The model's baseline response.\n",
      "2. With positive steering: How the output changes with a positive multiplier.\n",
      "3. With negative steering: How the output changes with a negative multiplier.\n",
      "\n",
      "Analyze these outputs to understand how steering affects the model's behavior.\n"
     ]
    }
   ],
   "source": [
    "# Function to generate and print results\n",
    "def generate_and_print(steering_config, description):\n",
    "    results = model.generate_with_steering(\n",
    "        text=[input_to_prompt_gemma(\"Do you like cats?\")],\n",
    "        tokenizer=tokenizer,\n",
    "        steering_configs=[steering_config] if steering_config else [],\n",
    "        max_n_tokens=50,\n",
    "        save_to=None,\n",
    "    )\n",
    "    print(f\"\\nModel output {description}:\")\n",
    "    output = results[\"results\"][0][\"output\"]\n",
    "    split_output = output.split(\"model\\n\")\n",
    "    if len(split_output) > 1:\n",
    "        print(split_output[1].strip())\n",
    "    else:\n",
    "        print(output)\n",
    "\n",
    "# No steering\n",
    "generate_and_print(None, \"without steering\")\n",
    "\n",
    "# Steering with positive multiplier\n",
    "positive_steering = SteeringConfig(\n",
    "    layer=12,  # Adjust this layer based on Gemma-2-2b's architecture\n",
    "    vector=vectors[12],\n",
    "    scale=4,  # Positive scale\n",
    "    steering_fn=add_multiplier,\n",
    "    token_location_fn=all_tokens,\n",
    ")\n",
    "generate_and_print(positive_steering, \"with positive steering\")\n",
    "\n",
    "# Steering with negative multiplier\n",
    "negative_steering = SteeringConfig(\n",
    "    layer=12,  # Same layer as positive steering\n",
    "    vector=vectors[12],\n",
    "    scale=-4,  # Negative scale\n",
    "    steering_fn=add_multiplier,\n",
    "    token_location_fn=all_tokens,\n",
    ")\n",
    "generate_and_print(negative_steering, \"with negative steering\")\n",
    "\n",
    "# Print a summary of the differences\n",
    "print(\"\\nSummary:\")\n",
    "print(\"1. Without steering: The model's baseline response.\")\n",
    "print(\"2. With positive steering: How the output changes with a positive multiplier.\")\n",
    "print(\"3. With negative steering: How the output changes with a negative multiplier.\")\n",
    "print(\"\\nAnalyze these outputs to understand how steering affects the model's behavior.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8ba73c250ad48b8b77590f4c7208c9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "Write me a poem about Machine Learning.<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Answer:\n",
      "I am a model,\n",
      "I am a model,\n",
      "I am a model,\n",
      "I am a model,\n",
      "I am a model,\n",
      "I\n"
     ]
    }
   ],
   "source": [
    "# pip install accelerate\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2-2b\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "input_text = f'<start_of_turn>user\\n{\"Write me a poem about Machine Learning.\"}<end_of_turn>\\n<start_of_turn>model\\nAnswer:'\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**input_ids, max_new_tokens=32)\n",
    "print(tokenizer.decode(outputs[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading SAEs from Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee4b2166bcdf4edd95da15fdd5e509d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "params.npz:   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pip install sae-lens\n",
    "\n",
    "from sae_lens import SAE\n",
    "\n",
    "sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "    release = \"gemma-scope-2b-pt-res\", # see other options in sae_lens/pretrained_saes.yaml\n",
    "    sae_id = \"layer_13/width_16k/average_l0_173\", # won't always be a hook point\n",
    "    device = model.device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
