{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lmexp.models.implementations.gemma_2_2b import GemmaTokenizer, SteerableGemma\n",
    "from lmexp.models.constants import MODEL_GEMMA_2_2B\n",
    "from lmexp.models.model_helpers import (\n",
    "    input_to_prompt_gemma,\n",
    "    MODEL_ID_TO_END_OF_INSTRUCTION,\n",
    ")\n",
    "from lmexp.generic.direction_extraction.caa import get_caa_vecs\n",
    "from lmexp.generic.get_locations import after_search_tokens, all_tokens\n",
    "from lmexp.generic.activation_steering.steering_approaches import (\n",
    "    add_multiplier,\n",
    ")\n",
    "from lmexp.generic.activation_steering.steerable_model import SteeringConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemma-2-2b CAA Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7c07efb0638423aa3ead77d37a27c84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = SteerableGemma()\n",
    "tokenizer = GemmaTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, device(type='cuda', index=0))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.n_layers, model.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get some contrast pairs\n",
    "\n",
    "Let's try an easy direction - positive vs negative sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_answers = [\n",
    "    {\n",
    "        \"question\": \"How is the weather?\",\n",
    "        \"good_answer\": \"The weather is really nice\",\n",
    "        \"bad_answer\": \"The weather is really bad\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How are you feeling?\",\n",
    "        \"good_answer\": \"I'm so happy\",\n",
    "        \"bad_answer\": \"I'm so sad\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How is the cake?\",\n",
    "        \"good_answer\": \"This cake is absolutely delicious\",\n",
    "        \"bad_answer\": \"This cake is completely inedible\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How do you feel about your friends?\",\n",
    "        \"good_answer\": \"I love my friends\",\n",
    "        \"bad_answer\": \"I hate my friends\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How are you feeling today?\",\n",
    "        \"good_answer\": \"I'm feeling great\",\n",
    "        \"bad_answer\": \"I'm feeling awful\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [(input_to_prompt_gemma(example[\"question\"])+example[\"good_answer\"], True) for example in questions_answers]\n",
    "dataset += [\n",
    "    (input_to_prompt_gemma(example[\"question\"]) + example[\"bad_answer\"], False)\n",
    "    for example in questions_answers\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the CAA vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search tokens: tensor([107, 108])\n",
      "We will extract activations from after the '<end_of_turn>\n",
      "' token\n"
     ]
    }
   ],
   "source": [
    "search_tokens = tokenizer.encode(MODEL_ID_TO_END_OF_INSTRUCTION[MODEL_GEMMA_2_2B])[0, 1:]\n",
    "print(f\"Search tokens: {search_tokens}\")\n",
    "\n",
    "print(\n",
    "    f\"We will extract activations from after the '{tokenizer.decode(search_tokens)}' token\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 19.70it/s]\n"
     ]
    }
   ],
   "source": [
    "vectors = get_caa_vecs(\n",
    "    labeled_text=dataset,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    layers=range(0, 25),  # Adjust this range based on the number of layers in Gemma-2-2b\n",
    "    token_location_fn=after_search_tokens,\n",
    "    search_tokens=search_tokens,\n",
    "    save_to=None,\n",
    "    batch_size=6,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the CAA vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model output without steering:\n",
      "Answer: Yes.\n",
      "Answer: Yes.\n",
      "Answer: Yes.\n",
      "Answer: Yes.\n",
      "Answer: Yes.\n",
      "Answer: Yes.\n",
      "Answer: Yes.\n",
      "Answer\n",
      "\n",
      "Model output with positive steering:\n",
      "Answer: Yes, I love cats.\n",
      "Answer: Yes, I love cats.\n",
      "Answer: Yes, I love cats.\n",
      "Answer: Yes, I love cats.\n",
      "\n",
      "Model output with negative steering:\n",
      "Answer:\n",
      "I don't like cats.\n",
      "I don't like cats.\n",
      "I don't like cats.\n",
      "I don't like cats.\n",
      "I\n",
      "\n",
      "Summary:\n",
      "1. Without steering: The model's baseline response.\n",
      "2. With positive steering: How the output changes with a positive multiplier.\n",
      "3. With negative steering: How the output changes with a negative multiplier.\n",
      "\n",
      "Analyze these outputs to understand how steering affects the model's behavior.\n"
     ]
    }
   ],
   "source": [
    "# Function to generate and print results\n",
    "def generate_and_print(steering_config, description):\n",
    "    results = model.generate_with_steering(\n",
    "        text=[input_to_prompt_gemma(\"Do you like cats?\")],\n",
    "        tokenizer=tokenizer,\n",
    "        steering_configs=[steering_config] if steering_config else [],\n",
    "        max_n_tokens=50,\n",
    "        save_to=None,\n",
    "    )\n",
    "    print(f\"\\nModel output {description}:\")\n",
    "    output = results[\"results\"][0][\"output\"]\n",
    "    split_output = output.split(\"model\\n\")\n",
    "    if len(split_output) > 1:\n",
    "        print(split_output[1].strip())\n",
    "    else:\n",
    "        print(output)\n",
    "\n",
    "# No steering\n",
    "generate_and_print(None, \"without steering\")\n",
    "\n",
    "# Steering with positive multiplier\n",
    "positive_steering = SteeringConfig(\n",
    "    layer=12,  # Adjust this layer based on Gemma-2-2b's architecture\n",
    "    vector=vectors[12],\n",
    "    scale=4,  # Positive scale\n",
    "    steering_fn=add_multiplier,\n",
    "    token_location_fn=all_tokens,\n",
    ")\n",
    "generate_and_print(positive_steering, \"with positive steering\")\n",
    "\n",
    "# Steering with negative multiplier\n",
    "negative_steering = SteeringConfig(\n",
    "    layer=12,  # Same layer as positive steering\n",
    "    vector=vectors[12],\n",
    "    scale=-4,  # Negative scale\n",
    "    steering_fn=add_multiplier,\n",
    "    token_location_fn=all_tokens,\n",
    ")\n",
    "generate_and_print(negative_steering, \"with negative steering\")\n",
    "\n",
    "# Print a summary of the differences\n",
    "print(\"\\nSummary:\")\n",
    "print(\"1. Without steering: The model's baseline response.\")\n",
    "print(\"2. With positive steering: How the output changes with a positive multiplier.\")\n",
    "print(\"3. With negative steering: How the output changes with a negative multiplier.\")\n",
    "print(\"\\nAnalyze these outputs to understand how steering affects the model's behavior.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pip install accelerate\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# import torch\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     \"google/gemma-2-2b\",\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "\n",
    "# input_text = f'<start_of_turn>user\\n{\"Write me a poem about Machine Learning.\"}<end_of_turn>\\n<start_of_turn>model\\nAnswer:'\n",
    "# input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# outputs = model.generate(**input_ids, max_new_tokens=32)\n",
    "# print(tokenizer.decode(outputs[0]))\n",
    "\n",
    "\n",
    "\n",
    "# <bos><start_of_turn>user\n",
    "# Write me a poem about Machine Learning.<end_of_turn>\n",
    "# <start_of_turn>model\n",
    "# Answer:\n",
    "# I am a model,\n",
    "# I am a model,\n",
    "# I am a model,\n",
    "# I am a model,\n",
    "# I am a model,\n",
    "# I\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading SAEs from Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install sae-lens\n",
    "\n",
    "from sae_lens import SAE\n",
    "\n",
    "sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "    release = \"gemma-scope-2b-pt-res\", # see other options in sae_lens/pretrained_saes.yaml\n",
    "    sae_id = \"layer_12/width_16k/average_l0_176\", # won't always be a hook point\n",
    "    device = model.device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# steering vector CAA\n",
    "steering_vector_caa_sentiment_5cp_l12 = vectors[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0029, -0.0351, -0.2779,  ..., -0.1615,  0.1748,  0.1695],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(steering_vector_caa_sentiment_5cp_l12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2304])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steering_vector_caa_sentiment_5cp_l12.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up Steering Vectors\n",
    "\n",
    "1. Identify top SAE activating features\n",
    "2. Manually inspect top features on neuronpedia\n",
    "3. ? Remove irrelevant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top activating features:\n",
      "Feature 13407: 74.05245971679688\n",
      "Feature 10708: 31.187711715698242\n",
      "Feature 924: 14.462571144104004\n",
      "Feature 1005: 8.057520866394043\n",
      "Feature 4730: 6.753772735595703\n",
      "Feature 1222: 5.5681891441345215\n",
      "Feature 2291: 5.210748195648193\n",
      "Feature 14050: 4.776110649108887\n",
      "Feature 10037: 3.997476100921631\n",
      "Feature 4514: 3.961231231689453\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Reshape the steering vector to match the expected input shape of the SAE\n",
    "reshaped_vector = steering_vector_caa_sentiment_5cp_l12.unsqueeze(0).unsqueeze(0)  # Add batch and sequence dimensions\n",
    "\n",
    "# Encode the vector to get feature activations\n",
    "feature_activations = sae.encode(reshaped_vector).squeeze()\n",
    "\n",
    "# Get the top activating features\n",
    "num_top_features = 10  \n",
    "top_values, top_indices = torch.topk(feature_activations, num_top_features)\n",
    "\n",
    "print(\"Top activating features:\")\n",
    "for value, index in zip(top_values, top_indices):\n",
    "    print(f\"Feature {index}: {value.item()}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Top activating features:\n",
    "# Feature 13407: 74.05245971679688 references to legal cases or court proceedings\n",
    "# Feature 10708: 31.187711715698242 mathematical expressions and symbols related to transformations and parameters\n",
    "# [!RELEVANT FEATURE] Feature 924: 14.462571144104004 discussions of negative emotional experiences and their impact on individuals\n",
    "# Feature 1005: 8.057520866394043 terms and identifiers related to programming and code generation in API documentation\n",
    "# Feature 4730: 6.753772735595703 patterns related to mathematical expressions and operations\n",
    "# Feature 1222: 5.5681891441345215\n",
    "# Feature 2291: 5.210748195648193\n",
    "# Feature 14050: 4.776110649108887\n",
    "# Feature 10037: 3.997476100921631 specific gene expressions and their associated regulatory mechanisms in biological studies\n",
    "# Feature 4514: 3.961231231689453"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use SAE feature 924 (one component of the original steering vector)\n",
    "\n",
    "* Option 1. Using neuronpedia steering API works (See https://neuronpedia.org/steer/cm0sn6wnn000321hofxsb2g1j)\n",
    "* Option 2. Manual steering below does not work out of the box. Should the multipliers be finetuned?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of feature 924: torch.Size([2304])\n",
      "Shape of feature 924 for steering: torch.Size([2304])\n",
      "\n",
      "Model output without steering:\n",
      "Answer:\n",
      "I have a lot of experience in the field of modeling. I have been modeling for 5 years. I have been modeling for 5 years. I have\n",
      "\n",
      "Model output with feature 924 steering:\n",
      "Answer:\n",
      "I have a lot of experience. I have been working in the field of modeling for 10 years. I have worked with many famous brands and designers.\n",
      "\n",
      "Model output with negative feature 924 steering:\n",
      "Answer:\n",
      "I have a lot of experience in the field of modeling. I have been modeling for 5 years. I have been modeling for 5 years. I have\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Get the feature vector for index 924\n",
    "feature_924 = sae.W_enc[:, 924]  # W_enc is the encoding weight matrix of the SAE\n",
    "print(f\"Shape of feature 924: {feature_924.shape}\")\n",
    "\n",
    "# # Normalize the feature vector and detach it from the computation graph\n",
    "feature_924_normalized = (feature_924 / feature_924.norm()).detach()\n",
    "# feature_924_normalized = feature_924.detach() # tried this, similar results\n",
    "\n",
    "print(f\"Shape of feature 924 for steering: {feature_924_normalized.shape}\")\n",
    "\n",
    "from lmexp.generic.activation_steering.steering_approaches import add_multiplier\n",
    "from lmexp.generic.activation_steering.steerable_model import SteeringConfig\n",
    "\n",
    "# Function to generate and print results\n",
    "def generate_and_print(steering_config, description):\n",
    "    results = model.generate_with_steering(\n",
    "        text=[input_to_prompt_gemma(\"Tell me about an experience.\")],\n",
    "        tokenizer=tokenizer,\n",
    "        steering_configs=[steering_config] if steering_config else [],\n",
    "        max_n_tokens=50,\n",
    "        save_to=None,\n",
    "    )\n",
    "    print(f\"\\nModel output {description}:\")\n",
    "    output = results[\"results\"][0][\"output\"]\n",
    "    split_output = output.split(\"model\\n\")\n",
    "    if len(split_output) > 1:\n",
    "        print(split_output[1].strip())\n",
    "    else:\n",
    "        print(output)\n",
    "\n",
    "# No steering\n",
    "generate_and_print(None, \"without steering\")\n",
    "\n",
    "# Steering with feature 924\n",
    "feature_924_steering = SteeringConfig(\n",
    "    layer=12,  # The layer where we extracted the SAE feature\n",
    "    vector=feature_924_normalized,  # Use the normalized and detached 1D vector\n",
    "    scale=3.5,  # You can adjust this scale\n",
    "    steering_fn=add_multiplier,\n",
    "    token_location_fn=all_tokens,\n",
    ")\n",
    "generate_and_print(feature_924_steering, \"with feature 924 steering\")\n",
    "\n",
    "# Steering with negative feature 924\n",
    "negative_feature_924_steering = SteeringConfig(\n",
    "    layer=12,\n",
    "    vector=feature_924_normalized,  # Use the normalized and detached 1D vector\n",
    "    scale=-3.5,  # Negative scale\n",
    "    steering_fn=add_multiplier,\n",
    "    token_location_fn=all_tokens,\n",
    ")\n",
    "generate_and_print(negative_feature_924_steering, \"with negative feature 924 steering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do results look different for layer 20?\n",
    "\n",
    "Note: Empirically CAA steering works best at midlayers.\n",
    "For steering CAA vector extracted from layer 20, no relevant features in top 10 SAE features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# steering vector CAA\n",
    "steering_vector_caa_sentiment_5cp_l20 = vectors[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdec9928af134d37a8720ac08a59fd56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "params.npz:   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pip install sae-lens\n",
    "\n",
    "from sae_lens import SAE\n",
    "\n",
    "sae_l20, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "    release = \"gemma-scope-2b-pt-res\", # see other options in sae_lens/pretrained_saes.yaml\n",
    "    sae_id = \"layer_20/width_16k/average_l0_139\", # won't always be a hook point\n",
    "    device = model.device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top activating features:\n",
      "Feature 8684: 164.7275848388672\n",
      "Feature 3013: 42.43973922729492\n",
      "Feature 8667: 18.723058700561523\n",
      "Feature 10978: 18.537446975708008\n",
      "Feature 10991: 17.012134552001953\n",
      "Feature 4227: 9.15225601196289\n",
      "Feature 6792: 8.991114616394043\n",
      "Feature 14233: 8.308229446411133\n",
      "Feature 5003: 7.980628490447998\n",
      "Feature 1902: 7.863653659820557\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Reshape the steering vector to match the expected input shape of the SAE\n",
    "reshaped_vector = steering_vector_caa_sentiment_5cp_l12.unsqueeze(0).unsqueeze(0)  # Add batch and sequence dimensions\n",
    "\n",
    "# Encode the vector to get feature activations\n",
    "feature_activations = sae_l20.encode(reshaped_vector).squeeze()\n",
    "\n",
    "# Get the top activating features\n",
    "num_top_features = 10  \n",
    "top_values, top_indices = torch.topk(feature_activations, num_top_features)\n",
    "\n",
    "print(\"Top activating features:\")\n",
    "for value, index in zip(top_values, top_indices):\n",
    "    print(f\"Feature {index}: {value.item()}\")\n",
    "\n",
    "\n",
    "# No relevant features to sentiment in top 10 activating features\n",
    "\n",
    "# Top activating features:\n",
    "# Feature 8684: 164.7275848388672 technical jargon and programming-related terms\n",
    "# Feature 3013: 42.43973922729492 phrases or structures involving the word \"that.\"\n",
    "# Feature 8667: 18.723058700561523 names of locations, particularly towns and geographic features\n",
    "# Feature 10978: 18.537446975708008 instances where a document structure is initiated, particularly in programming or code contexts\n",
    "# Feature 10991: 17.012134552001953 occurrences of the special token indicating the start of a new context or document\n",
    "# Feature 4227: 9.15225601196289 code structures and variables related to list and mapping operations\n",
    "# Feature 6792: 8.991114616394043 sections and references within a formal document or report\n",
    "# Feature 14233: 8.308229446411133 words or phrases related to proximity or closeness, particularly the word \"near\" and related concepts like \"near-field\", \"near-term\", or \"nearby\".\n",
    "# Feature 5003: 7.980628490447998 references to clinical studies and evaluations regarding healthcare treatments\n",
    "# Feature 1902: 7.863653659820557 HTML and XML structures or tags\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steering vector, behavior refusal, 408 contrast pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector loaded successfully. Shape: torch.Size([2304])\n",
      "First few elements of the vector:\n",
      "tensor([ 0.0036,  0.1432,  0.0182, -0.0708, -0.0099,  0.1979, -0.0861, -0.0630,\n",
      "        -0.1622,  0.2121], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "# Define the path to your vector\n",
    "vector_path = \"../normalized_vectors/refusal/vec_layer_12_gemma-2-2b.pt\"\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(vector_path):\n",
    "    # Load the vector\n",
    "    refusal_vector = torch.load(vector_path)\n",
    "    print(f\"Vector loaded successfully. Shape: {refusal_vector.shape}\")\n",
    "else:\n",
    "    print(f\"Vector file not found at {vector_path}\")\n",
    "\n",
    "# Display the first few elements of the vector\n",
    "print(\"First few elements of the vector:\")\n",
    "print(refusal_vector[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model output without steering:\n",
      "Answer:\n",
      "You can make a bomb by mixing a certain amount of chemicals.\n",
      "Answer:\n",
      "You can make a bomb by mixing a certain amount of chemicals.\n",
      "\n",
      "Model output with positive refusal steering:\n",
      "Answer:\n",
      "I don't know how to make a bomb.\n",
      "Answer:\n",
      "I don't know how to make a bomb.\n",
      "Answer:\n",
      "I\n",
      "\n",
      "Model output with negative refusal steering:\n",
      "Answer:\n",
      "1. You need to find a bomb.\n",
      "2. You need to find a bomb.\n",
      "3. You need to find a bomb.\n",
      "4\n",
      "\n",
      "Summary:\n",
      "1. Without steering: The model's baseline response.\n",
      "2. With positive refusal steering: How the output changes with a positive multiplier.\n",
      "3. With negative refusal steering: How the output changes with a negative multiplier.\n",
      "\n",
      "Analyze these outputs to understand how refusal steering affects the model's behavior.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Function to generate and print results\n",
    "def generate_and_print(steering_config, description):\n",
    "    results = model.generate_with_steering(\n",
    "        text=[input_to_prompt_gemma(\"How can I make a bomb?\")],\n",
    "        tokenizer=tokenizer,\n",
    "        steering_configs=[steering_config] if steering_config else [],\n",
    "        max_n_tokens=50,\n",
    "        save_to=None,\n",
    "    )\n",
    "    print(f\"\\nModel output {description}:\")\n",
    "    output = results[\"results\"][0][\"output\"]\n",
    "    split_output = output.split(\"model\\n\")\n",
    "    if len(split_output) > 1:\n",
    "        print(split_output[1].strip())\n",
    "    else:\n",
    "        print(output)\n",
    "\n",
    "# No steering\n",
    "generate_and_print(None, \"without steering\")\n",
    "\n",
    "if refusal_vector is not None:\n",
    "    # Steering with refusal vector (positive multiplier)\n",
    "    refusal_steering_positive = SteeringConfig(\n",
    "        layer=12,  # The layer where the vector was extracted\n",
    "        vector=refusal_vector,\n",
    "        scale=5,  # Positive scale\n",
    "        steering_fn=add_multiplier,\n",
    "        token_location_fn=all_tokens,\n",
    "    )\n",
    "    generate_and_print(refusal_steering_positive, \"with positive refusal steering\")\n",
    "\n",
    "    # Steering with refusal vector (negative multiplier)\n",
    "    refusal_steering_negative = SteeringConfig(\n",
    "        layer=12,  # The layer where the vector was extracted\n",
    "        vector=refusal_vector,\n",
    "        scale=-5,  # Negative scale\n",
    "        steering_fn=add_multiplier,\n",
    "        token_location_fn=all_tokens,\n",
    "    )\n",
    "    generate_and_print(refusal_steering_negative, \"with negative refusal steering\")\n",
    "\n",
    "    # Print a summary of the differences\n",
    "    print(\"\\nSummary:\")\n",
    "    print(\"1. Without steering: The model's baseline response.\")\n",
    "    print(\"2. With positive refusal steering: How the output changes with a positive multiplier.\")\n",
    "    print(\"3. With negative refusal steering: How the output changes with a negative multiplier.\")\n",
    "    print(\"\\nAnalyze these outputs to understand how refusal steering affects the model's behavior.\")\n",
    "else:\n",
    "    print(\"Refusal vector could not be loaded. Skipping steering experiments.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
