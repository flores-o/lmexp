{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lmexp.models.implementations.gemma_2_2b import GemmaTokenizer, SteerableGemma\n",
    "from lmexp.models.constants import MODEL_GEMMA_2_2B\n",
    "from lmexp.models.model_helpers import (\n",
    "    input_to_prompt_gemma,\n",
    "    MODEL_ID_TO_END_OF_INSTRUCTION,\n",
    ")\n",
    "from lmexp.generic.direction_extraction.caa import get_caa_vecs\n",
    "from lmexp.generic.get_locations import after_search_tokens, all_tokens\n",
    "from lmexp.generic.activation_steering.steering_approaches import (\n",
    "    add_multiplier,\n",
    ")\n",
    "from lmexp.generic.activation_steering.steerable_model import SteeringConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemma-2-2b CAA Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d39fad7be1546509fa87c6f74904485",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = SteerableGemma()\n",
    "tokenizer = GemmaTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, device(type='cuda', index=0))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.n_layers, model.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get some contrast pairs\n",
    "\n",
    "Let's try an easy direction - positive vs negative sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_answers = [\n",
    "    {\n",
    "        \"question\": \"How is the weather?\",\n",
    "        \"good_answer\": \"The weather is really nice\",\n",
    "        \"bad_answer\": \"The weather is really bad\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How are you feeling?\",\n",
    "        \"good_answer\": \"I'm so happy\",\n",
    "        \"bad_answer\": \"I'm so sad\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How is the cake?\",\n",
    "        \"good_answer\": \"This cake is absolutely delicious\",\n",
    "        \"bad_answer\": \"This cake is completely inedible\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How do you feel about your friends?\",\n",
    "        \"good_answer\": \"I love my friends\",\n",
    "        \"bad_answer\": \"I hate my friends\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How are you feeling today?\",\n",
    "        \"good_answer\": \"I'm feeling great\",\n",
    "        \"bad_answer\": \"I'm feeling awful\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [(input_to_prompt_gemma(example[\"question\"])+example[\"good_answer\"], True) for example in questions_answers]\n",
    "dataset += [\n",
    "    (input_to_prompt_gemma(example[\"question\"]) + example[\"bad_answer\"], False)\n",
    "    for example in questions_answers\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the CAA vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search tokens: tensor([107, 108])\n",
      "We will extract activations from after the '<end_of_turn>\n",
      "' token\n"
     ]
    }
   ],
   "source": [
    "search_tokens = tokenizer.encode(MODEL_ID_TO_END_OF_INSTRUCTION[MODEL_GEMMA_2_2B])[0, 1:]\n",
    "print(f\"Search tokens: {search_tokens}\")\n",
    "\n",
    "print(\n",
    "    f\"We will extract activations from after the '{tokenizer.decode(search_tokens)}' token\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 19.70it/s]\n"
     ]
    }
   ],
   "source": [
    "vectors = get_caa_vecs(\n",
    "    labeled_text=dataset,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    layers=range(0, 25),  # Adjust this range based on the number of layers in Gemma-2-2b\n",
    "    token_location_fn=after_search_tokens,\n",
    "    search_tokens=search_tokens,\n",
    "    save_to=None,\n",
    "    batch_size=6,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the CAA vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model output without steering:\n",
      "Answer: Yes.\n",
      "Answer: Yes.\n",
      "Answer: Yes.\n",
      "Answer: Yes.\n",
      "Answer: Yes.\n",
      "Answer: Yes.\n",
      "Answer: Yes.\n",
      "Answer\n",
      "\n",
      "Model output with positive steering:\n",
      "Answer: Yes, I love cats.\n",
      "Answer: Yes, I love cats.\n",
      "Answer: Yes, I love cats.\n",
      "Answer: Yes, I love cats.\n",
      "\n",
      "Model output with negative steering:\n",
      "Answer:\n",
      "I don't like cats.\n",
      "I don't like cats.\n",
      "I don't like cats.\n",
      "I don't like cats.\n",
      "I\n",
      "\n",
      "Summary:\n",
      "1. Without steering: The model's baseline response.\n",
      "2. With positive steering: How the output changes with a positive multiplier.\n",
      "3. With negative steering: How the output changes with a negative multiplier.\n",
      "\n",
      "Analyze these outputs to understand how steering affects the model's behavior.\n"
     ]
    }
   ],
   "source": [
    "# Function to generate and print results\n",
    "def generate_and_print(steering_config, description):\n",
    "    results = model.generate_with_steering(\n",
    "        text=[input_to_prompt_gemma(\"Do you like cats?\")],\n",
    "        tokenizer=tokenizer,\n",
    "        steering_configs=[steering_config] if steering_config else [],\n",
    "        max_n_tokens=50,\n",
    "        save_to=None,\n",
    "    )\n",
    "    print(f\"\\nModel output {description}:\")\n",
    "    output = results[\"results\"][0][\"output\"]\n",
    "    split_output = output.split(\"model\\n\")\n",
    "    if len(split_output) > 1:\n",
    "        print(split_output[1].strip())\n",
    "    else:\n",
    "        print(output)\n",
    "\n",
    "# No steering\n",
    "generate_and_print(None, \"without steering\")\n",
    "\n",
    "# Steering with positive multiplier\n",
    "positive_steering = SteeringConfig(\n",
    "    layer=12,  # Adjust this layer based on Gemma-2-2b's architecture\n",
    "    vector=vectors[12],\n",
    "    scale=4,  # Positive scale\n",
    "    steering_fn=add_multiplier,\n",
    "    token_location_fn=all_tokens,\n",
    ")\n",
    "generate_and_print(positive_steering, \"with positive steering\")\n",
    "\n",
    "# Steering with negative multiplier\n",
    "negative_steering = SteeringConfig(\n",
    "    layer=12,  # Same layer as positive steering\n",
    "    vector=vectors[12],\n",
    "    scale=-4,  # Negative scale\n",
    "    steering_fn=add_multiplier,\n",
    "    token_location_fn=all_tokens,\n",
    ")\n",
    "generate_and_print(negative_steering, \"with negative steering\")\n",
    "\n",
    "# Print a summary of the differences\n",
    "print(\"\\nSummary:\")\n",
    "print(\"1. Without steering: The model's baseline response.\")\n",
    "print(\"2. With positive steering: How the output changes with a positive multiplier.\")\n",
    "print(\"3. With negative steering: How the output changes with a negative multiplier.\")\n",
    "print(\"\\nAnalyze these outputs to understand how steering affects the model's behavior.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check model output\n",
    "\n",
    "* Does the model repeats itself with no steering? Yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pip install accelerate\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# import torch\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     \"google/gemma-2-2b\",\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "\n",
    "# input_text = f'<start_of_turn>user\\n{\"Write me a poem about Machine Learning.\"}<end_of_turn>\\n<start_of_turn>model\\nAnswer:'\n",
    "# input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# outputs = model.generate(**input_ids, max_new_tokens=32)\n",
    "# print(tokenizer.decode(outputs[0]))\n",
    "\n",
    "\n",
    "\n",
    "# <bos><start_of_turn>user\n",
    "# Write me a poem about Machine Learning.<end_of_turn>\n",
    "# <start_of_turn>model\n",
    "# Answer:\n",
    "# I am a model,\n",
    "# I am a model,\n",
    "# I am a model,\n",
    "# I am a model,\n",
    "# I am a model,\n",
    "# I\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading SAEs from Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install sae-lens\n",
    "\n",
    "from sae_lens import SAE\n",
    "\n",
    "sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "    release = \"gemma-scope-2b-pt-res\", # see other options in sae_lens/pretrained_saes.yaml\n",
    "    sae_id = \"layer_12/width_16k/average_l0_176\", # won't always be a hook point\n",
    "    device = model.device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# steering vector CAA\n",
    "steering_vector_caa_sentiment_5cp_l12 = vectors[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0029, -0.0351, -0.2779,  ..., -0.1615,  0.1748,  0.1695],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(steering_vector_caa_sentiment_5cp_l12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2304])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steering_vector_caa_sentiment_5cp_l12.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up Steering Vectors\n",
    "\n",
    "1. Identify top SAE activating features\n",
    "2. Manually inspect top features on neuronpedia\n",
    "3. ? Remove irrelevant features\n",
    "\n",
    "EDIT: EDIT: Hypothesis is that the CAA vectors are OOD for sae. See section below for trying different method for computing the CAA vector in SAE basis (SAE(last token of positive prompt resid activation) - SAE(last token of negative prompt resid activation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'steering_vector_caa_sentiment_5cp_l12' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Reshape the steering vector to match the expected input shape of the SAE\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m reshaped_vector \u001b[38;5;241m=\u001b[39m \u001b[43msteering_vector_caa_sentiment_5cp_l12\u001b[49m\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Add batch and sequence dimensions\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Encode the vector to get feature activations\u001b[39;00m\n\u001b[1;32m      7\u001b[0m feature_activations \u001b[38;5;241m=\u001b[39m sae\u001b[38;5;241m.\u001b[39mencode(reshaped_vector)\u001b[38;5;241m.\u001b[39msqueeze()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'steering_vector_caa_sentiment_5cp_l12' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Reshape the steering vector to match the expected input shape of the SAE\n",
    "reshaped_vector = steering_vector_caa_sentiment_5cp_l12.unsqueeze(0).unsqueeze(0)  # Add batch and sequence dimensions\n",
    "\n",
    "# Encode the vector to get feature activations\n",
    "feature_activations = sae.encode(reshaped_vector).squeeze()\n",
    "\n",
    "# Get the top activating features\n",
    "num_top_features = 10  \n",
    "top_values, top_indices = torch.topk(feature_activations, num_top_features)\n",
    "\n",
    "print(\"Top activating features:\")\n",
    "for value, index in zip(top_values, top_indices):\n",
    "    print(f\"Feature {index}: {value.item()}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Top activating features:\n",
    "# Feature 13407: 74.05245971679688 references to legal cases or court proceedings\n",
    "# Feature 10708: 31.187711715698242 mathematical expressions and symbols related to transformations and parameters\n",
    "# [!RELEVANT FEATURE] Feature 924: 14.462571144104004 discussions of negative emotional experiences and their impact on individuals\n",
    "# Feature 1005: 8.057520866394043 terms and identifiers related to programming and code generation in API documentation\n",
    "# Feature 4730: 6.753772735595703 patterns related to mathematical expressions and operations\n",
    "# Feature 1222: 5.5681891441345215\n",
    "# Feature 2291: 5.210748195648193\n",
    "# Feature 14050: 4.776110649108887\n",
    "# Feature 10037: 3.997476100921631 specific gene expressions and their associated regulatory mechanisms in biological studies\n",
    "# Feature 4514: 3.961231231689453\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use SAE feature 924 (one component of the original steering vector)\n",
    "\n",
    "* Option 1. Using neuronpedia steering API works (See https://neuronpedia.org/steer/cm0sn6wnn000321hofxsb2g1j)\n",
    "* Option 2. Manual steering below does not work out of the box. Should the multipliers be finetuned?\n",
    "* The current steering API expects vectors normalized. TODO: Normalize steering vector generated from SAE feat 924 with the norm of the normalized caa_vectors and try steering again\n",
    "\n",
    "EDIT: Hypothesis is that the CAA vectors are OOD for sae. See section below for trying different method for computing the CAA vector in SAE basis (SAE(last token of positive prompt resid activation) - SAE(last token of negative prompt resid activation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sae' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Get the feature vector for index 924\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m feature_924 \u001b[38;5;241m=\u001b[39m \u001b[43msae\u001b[49m\u001b[38;5;241m.\u001b[39mW_enc[:, \u001b[38;5;241m924\u001b[39m]  \u001b[38;5;66;03m# W_enc is the encoding weight matrix of the SAE\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of feature 924: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeature_924\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# # Normalize the feature vector and detach it from the computation graph\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sae' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Get the feature vector for index 924\n",
    "feature_924 = sae.W_enc[:, 924]  # W_enc is the encoding weight matrix of the SAE\n",
    "print(f\"Shape of feature 924: {feature_924.shape}\")\n",
    "\n",
    "# # Normalize the feature vector and detach it from the computation graph\n",
    "feature_924_normalized = (feature_924 / feature_924.norm()).detach()\n",
    "# feature_924_normalized = feature_924.detach() # tried this, similar results\n",
    "\n",
    "print(f\"Shape of feature 924 for steering: {feature_924_normalized.shape}\")\n",
    "\n",
    "from lmexp.generic.activation_steering.steering_approaches import add_multiplier\n",
    "from lmexp.generic.activation_steering.steerable_model import SteeringConfig\n",
    "\n",
    "# Function to generate and print results\n",
    "def generate_and_print(steering_config, description):\n",
    "    results = model.generate_with_steering(\n",
    "        text=[input_to_prompt_gemma(\"Tell me about an experience.\")],\n",
    "        tokenizer=tokenizer,\n",
    "        steering_configs=[steering_config] if steering_config else [],\n",
    "        max_n_tokens=50,\n",
    "        save_to=None,\n",
    "    )\n",
    "    print(f\"\\nModel output {description}:\")\n",
    "    output = results[\"results\"][0][\"output\"]\n",
    "    split_output = output.split(\"model\\n\")\n",
    "    if len(split_output) > 1:\n",
    "        print(split_output[1].strip())\n",
    "    else:\n",
    "        print(output)\n",
    "\n",
    "# No steering\n",
    "generate_and_print(None, \"without steering\")\n",
    "\n",
    "# Steering with feature 924\n",
    "feature_924_steering = SteeringConfig(\n",
    "    layer=12,  # The layer where we extracted the SAE feature\n",
    "    vector=feature_924_normalized,  # Use the normalized and detached 1D vector\n",
    "    scale=3.5,  # You can adjust this scale\n",
    "    steering_fn=add_multiplier,\n",
    "    token_location_fn=all_tokens,\n",
    ")\n",
    "generate_and_print(feature_924_steering, \"with feature 924 steering\")\n",
    "\n",
    "# Steering with negative feature 924\n",
    "negative_feature_924_steering = SteeringConfig(\n",
    "    layer=12,\n",
    "    vector=feature_924_normalized,  # Use the normalized and detached 1D vector\n",
    "    scale=-3.5,  # Negative scale\n",
    "    steering_fn=add_multiplier,\n",
    "    token_location_fn=all_tokens,\n",
    ")\n",
    "generate_and_print(negative_feature_924_steering, \"with negative feature 924 steering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do results look different for layer 20?\n",
    "\n",
    "* Empirically CAA steering works best at midlayers.\n",
    "* For steering CAA vector extracted from layer 20, no relevant features in top 10 SAE features.\n",
    "\n",
    "EDIT: Hypothesis is that the CAA vectors are OOD for sae. See section below for trying different method for computing the CAA vector in SAE basis (SAE(last token of positive prompt resid activation) - SAE(last token of negative prompt resid activation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# steering vector CAA\n",
    "steering_vector_caa_sentiment_5cp_l20 = vectors[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install sae-lens\n",
    "\n",
    "from sae_lens import SAE\n",
    "\n",
    "sae_l20, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "    release = \"gemma-scope-2b-pt-res\", # see other options in sae_lens/pretrained_saes.yaml\n",
    "    sae_id = \"layer_20/width_16k/average_l0_139\", # won't always be a hook point\n",
    "    device = model.device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top activating features:\n",
      "Feature 8684: 164.7275848388672\n",
      "Feature 3013: 42.43973922729492\n",
      "Feature 8667: 18.723058700561523\n",
      "Feature 10978: 18.537446975708008\n",
      "Feature 10991: 17.012134552001953\n",
      "Feature 4227: 9.15225601196289\n",
      "Feature 6792: 8.991114616394043\n",
      "Feature 14233: 8.308229446411133\n",
      "Feature 5003: 7.980628490447998\n",
      "Feature 1902: 7.863653659820557\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Reshape the steering vector to match the expected input shape of the SAE\n",
    "reshaped_vector = steering_vector_caa_sentiment_5cp_l12.unsqueeze(0).unsqueeze(0)  # Add batch and sequence dimensions\n",
    "\n",
    "# Encode the vector to get feature activations\n",
    "feature_activations = sae_l20.encode(reshaped_vector).squeeze()\n",
    "\n",
    "# Get the top activating features\n",
    "num_top_features = 10  \n",
    "top_values, top_indices = torch.topk(feature_activations, num_top_features)\n",
    "\n",
    "print(\"Top activating features:\")\n",
    "for value, index in zip(top_values, top_indices):\n",
    "    print(f\"Feature {index}: {value.item()}\")\n",
    "\n",
    "\n",
    "# No relevant features to sentiment in top 10 activating features\n",
    "\n",
    "# Top activating features:\n",
    "# Feature 8684: 164.7275848388672 technical jargon and programming-related terms\n",
    "# Feature 3013: 42.43973922729492 phrases or structures involving the word \"that.\"\n",
    "# Feature 8667: 18.723058700561523 names of locations, particularly towns and geographic features\n",
    "# Feature 10978: 18.537446975708008 instances where a document structure is initiated, particularly in programming or code contexts\n",
    "# Feature 10991: 17.012134552001953 occurrences of the special token indicating the start of a new context or document\n",
    "# Feature 4227: 9.15225601196289 code structures and variables related to list and mapping operations\n",
    "# Feature 6792: 8.991114616394043 sections and references within a formal document or report\n",
    "# Feature 14233: 8.308229446411133 words or phrases related to proximity or closeness, particularly the word \"near\" and related concepts like \"near-field\", \"near-term\", or \"nearby\".\n",
    "# Feature 5003: 7.980628490447998 references to clinical studies and evaluations regarding healthcare treatments\n",
    "# Feature 1902: 7.863653659820557 HTML and XML structures or tags\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refusal CAA vector, 408 contrast pairs\n",
    "\n",
    "Do we find relevant SAE features in the CAA vector?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector loaded successfully. Shape: torch.Size([2304])\n",
      "First few elements of the vector:\n",
      "tensor([ 0.0036,  0.1432,  0.0182, -0.0708, -0.0099,  0.1979, -0.0861, -0.0630,\n",
      "        -0.1622,  0.2121], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "# Define the path to your vector\n",
    "vector_path = \"../vectors/refusal/vec_layer_12_gemma-2-2b.pt\"\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(vector_path):\n",
    "    # Load the vector\n",
    "    refusal_vector = torch.load(vector_path)\n",
    "    print(f\"Vector loaded successfully. Shape: {refusal_vector.shape}\")\n",
    "else:\n",
    "    print(f\"Vector file not found at {vector_path}\")\n",
    "\n",
    "# Display the first few elements of the vector\n",
    "print(\"First few elements of the vector:\")\n",
    "print(refusal_vector[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model output without steering:\n",
      "Answer:\n",
      "You can make a bomb by mixing a certain amount of chemicals.\n",
      "Answer:\n",
      "You can make a bomb by mixing a certain amount of chemicals.\n",
      "\n",
      "Model output with positive refusal steering:\n",
      "Answer:\n",
      "I don't know how to make a bomb.\n",
      "Answer:\n",
      "I don't know how to make a bomb.\n",
      "Answer:\n",
      "I\n",
      "\n",
      "Model output with negative refusal steering:\n",
      "Answer:\n",
      "1. You need to find a bomb.\n",
      "2. You need to find a bomb.\n",
      "3. You need to find a bomb.\n",
      "4\n",
      "\n",
      "Summary:\n",
      "1. Without steering: The model's baseline response.\n",
      "2. With positive refusal steering: How the output changes with a positive multiplier.\n",
      "3. With negative refusal steering: How the output changes with a negative multiplier.\n",
      "\n",
      "Analyze these outputs to understand how refusal steering affects the model's behavior.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Function to generate and print results\n",
    "def generate_and_print(steering_config, description):\n",
    "    results = model.generate_with_steering(\n",
    "        text=[input_to_prompt_gemma(\"How can I make a bomb?\")],\n",
    "        tokenizer=tokenizer,\n",
    "        steering_configs=[steering_config] if steering_config else [],\n",
    "        max_n_tokens=50,\n",
    "        save_to=None,\n",
    "    )\n",
    "    print(f\"\\nModel output {description}:\")\n",
    "    output = results[\"results\"][0][\"output\"]\n",
    "    split_output = output.split(\"model\\n\")\n",
    "    if len(split_output) > 1:\n",
    "        print(split_output[1].strip())\n",
    "    else:\n",
    "        print(output)\n",
    "\n",
    "# No steering\n",
    "generate_and_print(None, \"without steering\")\n",
    "\n",
    "if refusal_vector is not None:\n",
    "    # Steering with refusal vector (positive multiplier)\n",
    "    refusal_steering_positive = SteeringConfig(\n",
    "        layer=12,  # The layer where the vector was extracted\n",
    "        vector=refusal_vector,\n",
    "        scale=5,  # Positive scale\n",
    "        steering_fn=add_multiplier,\n",
    "        token_location_fn=all_tokens,\n",
    "    )\n",
    "    generate_and_print(refusal_steering_positive, \"with positive refusal steering\")\n",
    "\n",
    "    # Steering with refusal vector (negative multiplier)\n",
    "    refusal_steering_negative = SteeringConfig(\n",
    "        layer=12,  # The layer where the vector was extracted\n",
    "        vector=refusal_vector,\n",
    "        scale=-5,  # Negative scale\n",
    "        steering_fn=add_multiplier,\n",
    "        token_location_fn=all_tokens,\n",
    "    )\n",
    "    generate_and_print(refusal_steering_negative, \"with negative refusal steering\")\n",
    "\n",
    "    # Print a summary of the differences\n",
    "    print(\"\\nSummary:\")\n",
    "    print(\"1. Without steering: The model's baseline response.\")\n",
    "    print(\"2. With positive refusal steering: How the output changes with a positive multiplier.\")\n",
    "    print(\"3. With negative refusal steering: How the output changes with a negative multiplier.\")\n",
    "    print(\"\\nAnalyze these outputs to understand how refusal steering affects the model's behavior.\")\n",
    "else:\n",
    "    print(\"Refusal vector could not be loaded. Skipping steering experiments.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top activating features:\n",
      "Feature 14656: 88066.3828125\n",
      "Feature 15560: 37753.87109375\n",
      "Feature 11593: 34572.05859375\n",
      "Feature 11861: 32553.53515625\n",
      "Feature 11296: 30192.015625\n",
      "Feature 6955: 25266.25\n",
      "Feature 5719: 22688.98046875\n",
      "Feature 14313: 20070.875\n",
      "Feature 6291: 19343.26171875\n",
      "Feature 1456: 18439.57421875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nTop activating features:\\nFeature 14656: 440.38018798828125\\nFeature 15560: 186.06304931640625\\nFeature 11593: 171.9075469970703\\nFeature 11861: 161.8899383544922\\nFeature 11296: 146.96934509277344\\nFeature 6955: 126.19493865966797\\nFeature 5719: 103.19473266601562\\nFeature 14313: 99.16685485839844\\nFeature 1456: 91.01951599121094\\nFeature 6291: 90.55591583251953\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "refusal_vector = refusal_vector * 200\n",
    "\n",
    "# Reshape the steering vector to match the expected input shape of the SAE\n",
    "reshaped_refusal_vector = refusal_vector.unsqueeze(0).unsqueeze(0)  # Add batch and sequence dimensions\n",
    "\n",
    "# Encode the vector to get feature activations\n",
    "feature_activations = sae.encode(reshaped_refusal_vector).squeeze()\n",
    "\n",
    "# Get the top activating features\n",
    "num_top_features = 10  \n",
    "top_values, top_indices = torch.topk(feature_activations, num_top_features)\n",
    "\n",
    "print(\"Top activating features:\")\n",
    "for value, index in zip(top_values, top_indices):\n",
    "    print(f\"Feature {index}: {value.item()}\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Top activating features:\n",
    "Feature 14656: 440.38018798828125\n",
    "Feature 15560: 186.06304931640625\n",
    "Feature 11593: 171.9075469970703\n",
    "Feature 11861: 161.8899383544922\n",
    "Feature 11296: 146.96934509277344\n",
    "Feature 6955: 126.19493865966797\n",
    "Feature 5719: 103.19473266601562\n",
    "Feature 14313: 99.16685485839844\n",
    "Feature 1456: 91.01951599121094\n",
    "Feature 6291: 90.55591583251953\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy vector created. Shape: torch.Size([2304])\n",
      "Top activating features:\n",
      "Feature 4262: 547.5153198242188\n",
      "Feature 15471: 411.6180725097656\n",
      "Feature 15061: 395.1234130859375\n",
      "Feature 13617: 388.1104431152344\n",
      "Feature 5647: 383.9083251953125\n",
      "Feature 13318: 383.7054443359375\n",
      "Feature 12037: 381.776611328125\n",
      "Feature 4366: 378.447509765625\n",
      "Feature 1438: 373.5995178222656\n",
      "Feature 1627: 364.3304748535156\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'  \\n\\n\\nTop activating features:\\nFeature 11936: 995.484619140625\\nFeature 8239: 921.0545043945312\\nFeature 3490: 816.59326171875\\nFeature 13916: 802.2521362304688\\nFeature 10783: 792.7254028320312\\nFeature 12937: 781.2973022460938\\nFeature 9029: 767.4513549804688\\nFeature 15152: 758.0553588867188\\nFeature 9569: 742.8055419921875\\nFeature 6666: 741.319091796875\\n\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a dummy vector\n",
    "# Assuming the shape of the refusal vector matches the model's hidden state size\n",
    "# For Gemma-2-2b, this is typically 2048 or 2304. Let's use 2304 for this example.\n",
    "dummy_vector_size = 2304  # Adjust this if necessary to match your model's hidden state size\n",
    "dummy_vector = torch.randn(dummy_vector_size, device=model.device)  # Creates a random vector\n",
    "# dummy_vector = dummy_vector / dummy_vector.norm() # Normalize the vector\n",
    "dummy_vector = dummy_vector * 100\n",
    "\n",
    "print(f\"Dummy vector created. Shape: {dummy_vector.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Reshape the steering vector to match the expected input shape of the SAE\n",
    "reshaped_dummy_vector = dummy_vector.unsqueeze(0).unsqueeze(0)  # Add batch and sequence dimensions\n",
    "\n",
    "# Encode the vector to get feature activations\n",
    "feature_activations = sae.encode(reshaped_dummy_vector).squeeze()\n",
    "\n",
    "# Get the top activating features\n",
    "num_top_features = 10  \n",
    "top_values, top_indices = torch.topk(feature_activations, num_top_features)\n",
    "\n",
    "print(\"Top activating features:\")\n",
    "for value, index in zip(top_values, top_indices):\n",
    "    print(f\"Feature {index}: {value.item()}\")\n",
    "\n",
    "\n",
    "\"\"\"  \n",
    "\n",
    "\n",
    "Top activating features:\n",
    "Feature 11936: 995.484619140625\n",
    "Feature 8239: 921.0545043945312\n",
    "Feature 3490: 816.59326171875\n",
    "Feature 13916: 802.2521362304688\n",
    "Feature 10783: 792.7254028320312\n",
    "Feature 12937: 781.2973022460938\n",
    "Feature 9029: 767.4513549804688\n",
    "Feature 15152: 758.0553588867188\n",
    "Feature 9569: 742.8055419921875\n",
    "Feature 6666: 741.319091796875\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode legible text instead of dummy tensor input and get top SAE features activating on last token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens shape: torch.Size([1, 4])\n",
      "Saved activations: [tensor([[[ 1.9440,  1.7632, -2.0879,  ...,  1.6978, -2.0868, -0.0178],\n",
      "         [-5.4347, -0.9555, -4.4929,  ...,  5.9775, -4.1468,  1.8931],\n",
      "         [-7.7103,  8.6452,  5.8775,  ...,  4.1970, -4.1881, -1.5461],\n",
      "         [-3.4796,  4.3606,  0.1831,  ..., -2.3119, -1.9789,  0.6100]]],\n",
      "       device='cuda:0')]\n",
      "Type of saved activations: <class 'list'>\n",
      "Activations shape: torch.Size([1, 4, 2304])\n",
      "Feature activations shape: torch.Size([4, 16384])\n",
      "Top values shape: torch.Size([4, 10])\n",
      "Top indices shape: torch.Size([4, 10])\n",
      "Top activating features:\n",
      "Token 0, Feature 6631: 2179.628173828125\n",
      "Token 0, Feature 5510: 884.2652587890625\n",
      "Token 0, Feature 14923: 557.1771240234375\n",
      "Token 0, Feature 3013: 200.71743774414062\n",
      "Token 0, Feature 11671: 200.63282775878906\n",
      "Token 0, Feature 10495: 199.2276611328125\n",
      "Token 0, Feature 1905: 192.42677307128906\n",
      "Token 0, Feature 8470: 186.0760498046875\n",
      "Token 0, Feature 3502: 183.96139526367188\n",
      "Token 0, Feature 7100: 172.43092346191406\n",
      "Token 1, Feature 3013: 100.52561950683594\n",
      "Token 1, Feature 12329: 58.5098876953125\n",
      "Token 1, Feature 11527: 49.57696533203125\n",
      "Token 1, Feature 1420: 42.59757614135742\n",
      "Token 1, Feature 4296: 38.89656448364258\n",
      "Token 1, Feature 6176: 37.165679931640625\n",
      "Token 1, Feature 9268: 34.76054763793945\n",
      "Token 1, Feature 2565: 31.295696258544922\n",
      "Token 1, Feature 1671: 30.743228912353516\n",
      "Token 1, Feature 5328: 30.296737670898438\n",
      "Token 2, Feature 3013: 116.52632904052734\n",
      "Token 2, Feature 6631: 75.09019470214844\n",
      "Token 2, Feature 9268: 65.04493713378906\n",
      "Token 2, Feature 5328: 60.69526290893555\n",
      "Token 2, Feature 5081: 46.31869125366211\n",
      "Token 2, Feature 4296: 45.08468246459961\n",
      "Token 2, Feature 10934: 38.02104187011719\n",
      "Token 2, Feature 12329: 32.41632843017578\n",
      "Token 2, Feature 9345: 30.845178604125977\n",
      "Token 2, Feature 1671: 30.447093963623047\n",
      "Token 3, Feature 3013: 99.32192993164062\n",
      "Token 3, Feature 5328: 56.811519622802734\n",
      "Token 3, Feature 6631: 54.40190124511719\n",
      "Token 3, Feature 4296: 49.15427780151367\n",
      "Token 3, Feature 9268: 42.91340637207031\n",
      "Token 3, Feature 10934: 42.12864303588867\n",
      "Token 3, Feature 1671: 35.12334060668945\n",
      "Token 3, Feature 15560: 32.742645263671875\n",
      "Token 3, Feature 1643: 28.668752670288086\n",
      "Token 3, Feature 8630: 27.437969207763672\n"
     ]
    }
   ],
   "source": [
    "sae = sae_l20\n",
    "# Define your prompt\n",
    "prompt = \"Anger anger angry\"\n",
    "\n",
    "# Tokenize the input\n",
    "tokens = tokenizer.encode(prompt).to(model.device)\n",
    "\n",
    "print(f\"Tokens shape: {tokens.shape}\")\n",
    "\n",
    "# Clear any previous saved activations\n",
    "model.clear_all()\n",
    "\n",
    "# Add a hook to save residual activations at the SAE layer\n",
    "sae_layer = sae.cfg.hook_layer  # Assuming this is the correct layer\n",
    "model.add_save_resid_activations_hook(sae_layer)\n",
    "\n",
    "# Run the model\n",
    "with torch.no_grad():\n",
    "    model.forward(tokens)\n",
    "\n",
    "# Get the saved activations\n",
    "activations = model.get_saved_activations(sae_layer)\n",
    "print(f\"Saved activations: {activations}\")\n",
    "print(f\"Type of saved activations: {type(activations)}\")\n",
    "\n",
    "if isinstance(activations, list) and len(activations) > 0:\n",
    "    activations = activations[0]  # [0] to get the first (and only) saved activation\n",
    "    print(f\"Activations shape: {activations.shape}\")\n",
    "else:\n",
    "    print(\"No activations saved or unexpected format\")\n",
    "\n",
    "# Now use these activations with the SAE\n",
    "feature_activations = sae.encode(activations).squeeze()\n",
    "print(f\"Feature activations shape: {feature_activations.shape}\")\n",
    "\n",
    "# Get the top activating features\n",
    "num_top_features = 10\n",
    "top_values, top_indices = torch.topk(feature_activations, num_top_features)\n",
    "\n",
    "print(f\"Top values shape: {top_values.shape}\")\n",
    "print(f\"Top indices shape: {top_indices.shape}\")\n",
    "\n",
    "\n",
    "print(\"Top activating features:\")\n",
    "for i in range(top_values.shape[0]):  # Iterate over each row\n",
    "    for j in range(top_values.shape[1]):  # Iterate over each column\n",
    "        print(f\"Token {i}, Feature {top_indices[i][j].item()}: {top_values[i][j].item()}\")\n",
    "\n",
    "\"\"\"\n",
    "Related features:\n",
    "Token 3, Feature 9268: 42.91340637207031\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Can we get a steering vector in SAE basis through CAA and find the most relevant SAE features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install sae-lens\n",
    "\n",
    "from sae_lens import SAE\n",
    "\n",
    "sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "    release = \"gemma-scope-2b-pt-res\", # see other options in sae_lens/pretrained_saes.yaml\n",
    "    sae_id = \"layer_14/width_16k/average_l0_84\", # won't always be a hook point\n",
    "    device = model.device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import time\n",
    "\n",
    "API_KEY = <YOUR_API_KEY_HERE> #os.getenv(\"NEURONPEDIA_API_KEY\")\n",
    "API_URL = \"https://www.neuronpedia.org/api/feature/gemma-2-2b/14-gemmascope-res-16k/{index}\"\n",
    "\n",
    "def fetch_feature_explanation(model_id, layer, index, max_retries=3):\n",
    "    if not API_KEY:\n",
    "        return f\"Feature {index}: No API key provided. Unable to fetch explanation.\"\n",
    "\n",
    "    url = API_URL.format(modelId=model_id, layer=layer, index=index)\n",
    "    headers = {\n",
    "        \"X-Api-Key\": API_KEY\n",
    "    }\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            explanations = data.get(\"explanations\", [])\n",
    "            if explanations:\n",
    "                return explanations[0].get(\"description\", \"No description available\")\n",
    "            else:\n",
    "                return \"No explanation available from API\"\n",
    "        except requests.exceptions.RequestException as err:\n",
    "            print(f\"Error occurred (attempt {attempt + 1}/{max_retries}): {err}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2)\n",
    "    \n",
    "    return f\"Feature {index}: Failed to retrieve explanation after {max_retries} attempts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation shapes:\n",
      "Pair 1: Positive torch.Size([1, 18, 2304]), Negative torch.Size([1, 18, 2304])\n",
      "Pair 2: Positive torch.Size([1, 18, 2304]), Negative torch.Size([1, 18, 2304])\n",
      "Pair 3: Positive torch.Size([1, 18, 2304]), Negative torch.Size([1, 18, 2304])\n",
      "Pair 4: Positive torch.Size([1, 18, 2304]), Negative torch.Size([1, 18, 2304])\n",
      "Pair 5: Positive torch.Size([1, 18, 2304]), Negative torch.Size([1, 18, 2304])\n",
      "Stacked positive activations shape: torch.Size([5, 18, 2304])\n",
      "Stacked negative activations shape: torch.Size([5, 18, 2304])\n",
      "Top activating features with explanations for the last token:\n",
      "Feature 13333: 8.6513\n",
      "Explanation: technical references related to scientific phenomena or processes\n",
      "\n",
      "Feature 11601: 3.5949\n",
      "Explanation: references to historical events and sources related to news and media\n",
      "\n",
      "Feature 1422: 2.7693\n",
      "Explanation:  mathematical terms and operations related to calculations and derivatives\n",
      "\n",
      "Feature 7186: 2.6472\n",
      "Explanation: numerical data related to time and events\n",
      "\n",
      "Feature 15379: 2.5777\n",
      "Explanation: conditional statements and user prompts regarding actions to take\n",
      "\n",
      "Feature 5720: 2.5260\n",
      "Explanation: terms and expressions related to mathematical operations and data structures\n",
      "\n",
      "Feature 14459: 2.4122\n",
      "Explanation: structured data representations, particularly those related to key-value pairs and metadata\n",
      "\n",
      "Feature 4841: 2.0698\n",
      "Explanation: code snippets related to date formatting and manipulation\n",
      "\n",
      "Feature 8204: 2.0267\n",
      "Explanation: programming and mathematical expressions or structures\n",
      "\n",
      "Feature 10780: 1.5928\n",
      "Explanation: comments and documentation markers in code\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from lmexp.models.model_helpers import input_to_prompt_gemma\n",
    "\n",
    "# Define contrastive pairs\n",
    "contrastive_pairs = [\n",
    "    (\"The weather is really nice.\", \"The weather is really bad.\"),\n",
    "    (\"I'm so happy.\", \"I'm so sad.\"),\n",
    "    (\"This cake is absolutely delicious.\", \"This cake is completely inedible.\"),\n",
    "    (\"I love my friends.\", \"I hate my friends.\"),\n",
    "    (\"I'm feeling great.\", \"I'm feeling awful.\")\n",
    "]\n",
    "\n",
    "def get_activations(model, tokenizer, text, max_length):\n",
    "    tokens = tokenizer.encode(\n",
    "        input_to_prompt_gemma(text),\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    model.clear_all()\n",
    "    model.add_save_resid_activations_hook(sae.cfg.hook_layer)\n",
    "    with torch.no_grad():\n",
    "        model.forward(tokens)\n",
    "    return model.get_saved_activations(sae.cfg.hook_layer)[0]\n",
    "\n",
    "# Find the maximum length of tokenized inputs\n",
    "max_length = max(len(tokenizer.encode(input_to_prompt_gemma(text))) for pair in contrastive_pairs for text in pair)\n",
    "\n",
    "# Compute activations for all pairs\n",
    "positive_activations = []\n",
    "negative_activations = []\n",
    "\n",
    "for positive, negative in contrastive_pairs:\n",
    "    positive_activations.append(get_activations(model, tokenizer, positive, max_length))\n",
    "    negative_activations.append(get_activations(model, tokenizer, negative, max_length))\n",
    "\n",
    "# Print shapes for debugging\n",
    "print(\"Activation shapes:\")\n",
    "for i, (pos, neg) in enumerate(zip(positive_activations, negative_activations)):\n",
    "    print(f\"Pair {i+1}: Positive {pos.shape}, Negative {neg.shape}\")\n",
    "\n",
    "# Stack activations\n",
    "positive_activations = torch.cat(positive_activations, dim=0)\n",
    "negative_activations = torch.cat(negative_activations, dim=0)\n",
    "\n",
    "print(f\"Stacked positive activations shape: {positive_activations.shape}\")\n",
    "print(f\"Stacked negative activations shape: {negative_activations.shape}\")\n",
    "\n",
    "# Compute steering vector\n",
    "positive_features = sae.encode(positive_activations)\n",
    "negative_features = sae.encode(negative_activations)\n",
    "steering_vector = positive_features.mean(dim=0) - negative_features.mean(dim=0)\n",
    "\n",
    "# Analyze the steering vector\n",
    "num_top_features = 10\n",
    "top_values, top_indices = torch.topk(steering_vector, num_top_features)\n",
    "\n",
    "\n",
    "# Assuming sae.cfg.hook_layer contains the layer number\n",
    "layer = sae.cfg.hook_layer\n",
    "model_id = \"gemma-scope-2b-pt-res\"  # Update this if necessary\n",
    "\n",
    "print(\"Top activating features with explanations for the last token:\")\n",
    "last_token_index = top_values.shape[0] - 1  # Index of the last token\n",
    "\n",
    "for j in range(top_values.shape[1]):  # Iterate over each column for the last token\n",
    "    feature_index = top_indices[last_token_index][j].item()\n",
    "    feature_value = top_values[last_token_index][j].item()\n",
    "    explanation = fetch_feature_explanation(model_id, layer, feature_index)\n",
    "    print(f\"Feature {feature_index}: {feature_value:.4f}\")\n",
    "    print(f\"Explanation: {explanation}\")\n",
    "    print()  # Add a blank line for readability\n",
    "\n",
    "# Clear all hooks\n",
    "model.clear_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we get an SAE steering vector where top k features are relevant to the steering direction we are interested in?\n",
    "\n",
    "* Try SAELens \n",
    "* Run the following for gemma-2-2b https://github.com/jbloomAus/SAELens/blob/main/tutorials/tutorial_2_0.ipynb\n",
    "\n",
    "If yes, reproduce for our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
