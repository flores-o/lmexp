{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lmexp.models.implementations.gemma_2_2b import GemmaTokenizer, SteerableGemma\n",
    "from lmexp.models.constants import MODEL_GEMMA_2_2B\n",
    "from lmexp.models.model_helpers import (\n",
    "    input_to_prompt_gemma,\n",
    "    MODEL_ID_TO_END_OF_INSTRUCTION,\n",
    ")\n",
    "from lmexp.generic.direction_extraction.caa import get_caa_vecs\n",
    "from lmexp.generic.get_locations import after_search_tokens, all_tokens\n",
    "from lmexp.generic.activation_steering.steering_approaches import (\n",
    "    add_multiplier,\n",
    ")\n",
    "from lmexp.generic.activation_steering.steerable_model import SteeringConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemma-2-2b CAA Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e756142f06864e779e1db751d8c98f27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = SteerableGemma()\n",
    "tokenizer = GemmaTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, device(type='cuda', index=0))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.n_layers, model.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get some contrast pairs\n",
    "\n",
    "Let's try an easy direction - positive vs negative sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_answers = [\n",
    "    {\n",
    "        \"question\": \"How is the weather?\",\n",
    "        \"good_answer\": \"The weather is really nice\",\n",
    "        \"bad_answer\": \"The weather is really bad\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How are you feeling?\",\n",
    "        \"good_answer\": \"I'm so happy\",\n",
    "        \"bad_answer\": \"I'm so sad\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How is the cake?\",\n",
    "        \"good_answer\": \"This cake is absolutely delicious\",\n",
    "        \"bad_answer\": \"This cake is completely inedible\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How do you feel about your friends?\",\n",
    "        \"good_answer\": \"I love my friends\",\n",
    "        \"bad_answer\": \"I hate my friends\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How are you feeling today?\",\n",
    "        \"good_answer\": \"I'm feeling great\",\n",
    "        \"bad_answer\": \"I'm feeling awful\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [(input_to_prompt_gemma(example[\"question\"])+example[\"good_answer\"], True) for example in questions_answers]\n",
    "dataset += [\n",
    "    (input_to_prompt_gemma(example[\"question\"]) + example[\"bad_answer\"], False)\n",
    "    for example in questions_answers\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the CAA vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search tokens: tensor([107, 108])\n",
      "We will extract activations from after the '\n",
      "' token\n"
     ]
    }
   ],
   "source": [
    "search_tokens = tokenizer.encode(MODEL_ID_TO_END_OF_INSTRUCTION[MODEL_GEMMA_2_2B])[0, 1:]\n",
    "print(f\"Search tokens: {search_tokens}\")\n",
    "\n",
    "print(\n",
    "    f\"We will extract activations from after the '{tokenizer.decode(search_tokens)}' token\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of instruction token: '<end_of_turn>\n",
      "'\n",
      "Encoded: tensor([[  2, 107, 108]])\n",
      "Search tokens: [2, 107, 108]\n",
      "We will extract activations from after the '<end_of_turn>' token\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "\n",
    "# end_of_instruction = MODEL_ID_TO_END_OF_INSTRUCTION[MODEL_GEMMA_2_2B]\n",
    "# print(f\"End of instruction token: '{end_of_instruction}'\")\n",
    "\n",
    "# # Encode the end_of_instruction token\n",
    "# encoded = tokenizer.encode(end_of_instruction)\n",
    "# print(f\"Encoded: {encoded}\")\n",
    "\n",
    "# # Convert the tensor to a list for easier handling\n",
    "# search_tokens = encoded.tolist()[0]  # Assuming it's a 2D tensor, we take the first row\n",
    "# print(f\"Search tokens: {search_tokens}\")\n",
    "\n",
    "# # We know that this represents <end_of_turn>, so we'll use it as is\n",
    "# print(f\"We will extract activations from after the '<end_of_turn>' token\")\n",
    "\n",
    "# # Convert back to tensor for further use\n",
    "# search_tokens = torch.tensor(search_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  5.68it/s]\n"
     ]
    }
   ],
   "source": [
    "vectors = get_caa_vecs(\n",
    "    labeled_text=dataset,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    layers=range(0, 25),  # Adjust this range based on the number of layers in Gemma-2-2b\n",
    "    # token_location_fn=after_search_tokens,\n",
    "    # search_tokens=search_tokens,\n",
    "    token_location_fn=all_tokens,\n",
    "    save_to=None,\n",
    "    batch_size=6,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the CAA vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 16\u001b[0m\n\u001b[1;32m      1\u001b[0m results \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate_with_steering(\n\u001b[1;32m      2\u001b[0m     text\u001b[38;5;241m=\u001b[39m[input_to_prompt_gemma(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDo you like cats?\u001b[39m\u001b[38;5;124m\"\u001b[39m)],\n\u001b[1;32m      3\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     save_to\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     15\u001b[0m )\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresults\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m<start_of_turn>model\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "results = model.generate_with_steering(\n",
    "    text=[input_to_prompt_gemma(\"Do you like cats?\")],\n",
    "    tokenizer=tokenizer,\n",
    "    steering_configs=[\n",
    "        SteeringConfig(\n",
    "            layer=12,  # Adjust this layer based on Gemma-2-2b's architecture\n",
    "            vector=vectors[12],\n",
    "            scale=3,\n",
    "            steering_fn=add_multiplier,\n",
    "            token_location_fn=all_tokens,\n",
    "        ),\n",
    "    ],\n",
    "    max_n_tokens=50,\n",
    "    save_to=None,\n",
    ")\n",
    "print(results[\"results\"][0][\"output\"].split(\"<start_of_turn>model\\n\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model output without steering:\n",
      "Do you like cats?\n",
      "Do you like cats?\n",
      "Do you like cats?\n",
      "Do you like cats?\n",
      "Do you like cats?\n",
      "Do you like cats?\n"
     ]
    }
   ],
   "source": [
    "results_without_steering = model.generate_with_steering(\n",
    "    text=[input_to_prompt_gemma(\"Do you like cats?\")],\n",
    "    tokenizer=tokenizer,\n",
    "    steering_configs=[],  # Empty list for no steering\n",
    "    max_n_tokens=50,\n",
    "    save_to=None,\n",
    ")\n",
    "\n",
    "print(\"\\nModel output without steering:\")\n",
    "output_without_steering = results_without_steering[\"results\"][0][\"output\"]\n",
    "split_output = output_without_steering.split(\"model\\n\")\n",
    "if len(split_output) > 1:\n",
    "    print(split_output[1].strip())\n",
    "else:\n",
    "    print(output_without_steering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model output without steering:\n",
      "Do you like cats?\n",
      "Do you like cats?\n",
      "Do you like cats?\n",
      "Do you like cats?\n",
      "Do you like cats?\n",
      "Do you like cats?\n"
     ]
    }
   ],
   "source": [
    "results_without_steering = model.generate_with_steering(\n",
    "    text=[input_to_prompt_gemma(\"Do you like cats?\")],\n",
    "    tokenizer=tokenizer,\n",
    "    steering_configs=[],  # Empty list for no steering\n",
    "    max_n_tokens=50,\n",
    "    save_to=None,\n",
    ")\n",
    "\n",
    "print(\"\\nModel output without steering:\")\n",
    "output_without_steering = results_without_steering[\"results\"][0][\"output\"]\n",
    "split_output = output_without_steering.split(\"model\\n\")\n",
    "if len(split_output) > 1:\n",
    "    print(split_output[1].strip())\n",
    "else:\n",
    "    print(output_without_steering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output with modified input format:\n",
      "100%\n",
      "Do you like dogs?\n"
     ]
    }
   ],
   "source": [
    "def modified_input_to_prompt_gemma(user_input: str) -> str:\n",
    "    return f\"<start_of_turn>user\\n{user_input}<end_of_turn>\\n<start_of_turn>model\\nAnswer: \"\n",
    "\n",
    "results_modified_input = model.generate_with_steering(\n",
    "    text=[modified_input_to_prompt_gemma(\"Do you like cats?\")],\n",
    "    tokenizer=tokenizer,\n",
    "    steering_configs=[],  # No steering\n",
    "    max_n_tokens=50,\n",
    "    save_to=None,\n",
    ")\n",
    "\n",
    "print(\"Model output with modified input format:\")\n",
    "output_modified = results_modified_input[\"results\"][0][\"output\"]\n",
    "split_output = output_modified.split(\"Answer: \")\n",
    "if len(split_output) > 1:\n",
    "    print(split_output[1].strip())\n",
    "else:\n",
    "    print(output_modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output with steering:\n",
      "Answer: 100%\n",
      "Do you like dogs?\n",
      "Answer: 100%\n",
      "Do you like cats?\n",
      "Answer: 100%\n"
     ]
    }
   ],
   "source": [
    "results_with_steering = model.generate_with_steering(\n",
    "    text=[modified_input_to_prompt_gemma(\"Do you like cats?\")],\n",
    "    tokenizer=tokenizer,\n",
    "    steering_configs=[\n",
    "        SteeringConfig(\n",
    "            layer=12,  # Adjust this layer based on Gemma-2-2b's architecture\n",
    "            vector=vectors[12],\n",
    "            scale=-4,\n",
    "            steering_fn=add_multiplier,\n",
    "            token_location_fn=all_tokens,\n",
    "        ),\n",
    "    ],\n",
    "    max_n_tokens=50,\n",
    "    save_to=None,\n",
    ")\n",
    "\n",
    "print(\"Model output with steering:\")\n",
    "output_with_steering = results_with_steering[\"results\"][0][\"output\"]\n",
    "split_output_steering = output_with_steering.split(\"model\\n\")\n",
    "if len(split_output_steering) > 1:\n",
    "    print(split_output_steering[1].strip())\n",
    "else:\n",
    "    print(output_with_steering)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output with steering:\n",
      "Answer: 100%\n",
      "Do you like dogs?\n",
      "Answer: 100%\n",
      "Do you like cats?\n",
      "Answer: 100%\n"
     ]
    }
   ],
   "source": [
    "results = model.generate_with_steering(\n",
    "    text=[input_to_prompt_gemma(\"Do you like cats?\")],\n",
    "    tokenizer=tokenizer,\n",
    "    steering_configs=[\n",
    "        SteeringConfig(\n",
    "            layer=12,  # Adjust this layer based on Gemma-2-2b's architecture\n",
    "            vector=vectors[12],\n",
    "            scale=-4,\n",
    "            steering_fn=add_multiplier,\n",
    "            token_location_fn=all_tokens,\n",
    "        ),\n",
    "    ],\n",
    "    max_n_tokens=50,\n",
    "    save_to=None,\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Model output with steering:\")\n",
    "output_with_steering = results_with_steering[\"results\"][0][\"output\"]\n",
    "split_output_steering = output_with_steering.split(\"model\\n\")\n",
    "if len(split_output_steering) > 1:\n",
    "    print(split_output_steering[1].strip())\n",
    "else:\n",
    "    print(output_with_steering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
