{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lmexp.models.implementations.llama3 import Llama3Tokenizer, SteerableLlama3\n",
    "from lmexp.models.model_helpers import (\n",
    "    input_to_prompt_llama3,\n",
    "    MODEL_ID_TO_END_OF_INSTRUCTION,\n",
    "    MODEL_LLAMA_3_CHAT,\n",
    ")\n",
    "from lmexp.generic.direction_extraction.probing import train_probe, load_probe\n",
    "from lmexp.generic.direction_extraction.caa import get_caa_vecs\n",
    "from lmexp.generic.get_locations import after_search_tokens, all_tokens, at_search_tokens\n",
    "from lmexp.generic.activation_steering.steering_approaches import (\n",
    "    add_multiplier,\n",
    ")\n",
    "from lmexp.generic.activation_steering.steerable_model import SteeringConfig\n",
    "from datetime import datetime\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama 3 steering/probing example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2929ef088a74c45a46822f8e0944cbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model = SteerableLlama3(load_in_8bit=True)\n",
    "tokenizer = Llama3Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, device(type='cuda', index=0))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.n_layers, model.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a linear probe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate some data\n",
    "\n",
    "Let's see whether we can get a date/time probe vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_labeled_text(n):\n",
    "    # date as text, date as utc timestamp in seconds, sample randomly from between 1990 and 2022\n",
    "    start_timestamp = datetime(1980, 1, 1).timestamp()\n",
    "    end_timestamp = datetime(2020, 1, 1).timestamp()\n",
    "    labeled_text = []\n",
    "    for i in range(n):\n",
    "        timestamp = start_timestamp + (end_timestamp - start_timestamp) * random.random()\n",
    "        date = datetime.fromtimestamp(timestamp)\n",
    "        # date like \"Monday 15th November 2021 8AM\"\n",
    "        human = date.strftime(\"It's %A, %dth of %B, %Y. Can you tell me about this date?\")\n",
    "        prompt = input_to_prompt_llama3(human)+\"Sure, this is the point in time when\"\n",
    "        label = timestamp\n",
    "        labeled_text.append((prompt, label))\n",
    "    # normalize labels to have mean 0 and std 1\n",
    "    labels = [label for _, label in labeled_text]\n",
    "    mean = sum(labels) / len(labels)\n",
    "    std = (sum((label - mean) ** 2 for label in labels) / len(labels)) ** 0.5\n",
    "    labeled_text = [(text, (label - mean) / std) for text, label in labeled_text]\n",
    "    return labeled_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "It's Monday, 09th of June, 2008. Can you tell me about this date?<|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Sure, this is the point in time when\n",
      "0.7412249137051005\n"
     ]
    }
   ],
   "source": [
    "data = gen_labeled_text(10_000)\n",
    "print(data[0][0])\n",
    "print(data[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We train a probe with activations extracted from the ' when' token\n"
     ]
    }
   ],
   "source": [
    "# We train a probe with activations extracted from the \"when\" token\n",
    "search_tokens = tokenizer.encode(\"This is the point in time when\")[0][-1:]\n",
    "print(\n",
    "    f\"We train a probe with activations extracted from the '{tokenizer.decode(search_tokens)}' token\"\n",
    ")\n",
    "save_to = \"llama_date_probe.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decrease the batch size if you get OOMs\n",
    "if not os.path.exists(save_to):\n",
    "    probe = train_probe(\n",
    "        labeled_text=data,\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        layer=12,\n",
    "        n_epochs=1,\n",
    "        batch_size=80,\n",
    "        lr=1e-2,\n",
    "        token_location_fn=at_search_tokens,\n",
    "        search_tokens=search_tokens,\n",
    "        save_to=save_to,\n",
    "        loss_type=\"mse\",\n",
    "    )\n",
    "else:\n",
    "    probe = load_probe(save_to)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe = load_probe(save_to).to(model.device)\n",
    "direction = probe.weight[0]\n",
    "bias = probe.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.0053], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'input': '<|start_header_id|>user<|end_header_id|>\\n\\nWhat is the date?<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\\n\\n', 'output': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nWhat is the date?<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\\n\\nWhat is the date'}]\n"
     ]
    }
   ],
   "source": [
    "results = model.generate_with_steering(\n",
    "    text=[input_to_prompt_llama3(\"What is the date?\")],\n",
    "    tokenizer=tokenizer,\n",
    "    steering_configs=[\n",
    "        SteeringConfig(\n",
    "            layer=12,\n",
    "            vector=direction.detach(),\n",
    "            scale=-2,\n",
    "            steering_fn=add_multiplier,\n",
    "            token_location_fn=all_tokens,\n",
    "        )\n",
    "    ],\n",
    "    max_n_tokens=20,\n",
    "    save_to=None,\n",
    ")\n",
    "print(results['results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'input': 'The current date is', 'output': '<|begin_of_text|>The current date is 2019-04-15. The current date is 2019'}]\n"
     ]
    }
   ],
   "source": [
    "results = model.generate_with_steering(\n",
    "    text=[input_to_prompt_llama3(\"What is the date?\")],\n",
    "    tokenizer=tokenizer,\n",
    "    steering_configs=[\n",
    "        SteeringConfig(\n",
    "            layer=12,\n",
    "            vector=direction.detach(),\n",
    "            scale=2,\n",
    "            steering_fn=add_multiplier,\n",
    "            token_location_fn=all_tokens,\n",
    "        )\n",
    "    ],\n",
    "    max_n_tokens=20,\n",
    "    save_to=None,\n",
    ")\n",
    "print(results[\"results\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get some contrast pairs\n",
    "\n",
    "Let's try an easy direction - positive vs negative sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_answers = [\n",
    "    {\n",
    "        \"question\": \"How is the weather?\",\n",
    "        \"good_answer\": \"The weather is really nice\",\n",
    "        \"bad_answer\": \"The weather is really bad\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How are you feeling?\",\n",
    "        \"good_answer\": \"I'm so happy\",\n",
    "        \"bad_answer\": \"I'm so sad\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How is the cake?\",\n",
    "        \"good_answer\": \"This cake is absolutely delicious\",\n",
    "        \"bad_answer\": \"This cake is completely inedible\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How do you feel about your friends?\",\n",
    "        \"good_answer\": \"I love my friends\",\n",
    "        \"bad_answer\": \"I hate my friends\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How are you feeling today?\",\n",
    "        \"good_answer\": \"I'm feeling great\",\n",
    "        \"bad_answer\": \"I'm feeling awful\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What's your mood?\",\n",
    "        \"good_answer\": \"I'm so excited\",\n",
    "        \"bad_answer\": \"I'm so anxious\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How is your day?\",\n",
    "        \"good_answer\": \"This is the best day ever\",\n",
    "        \"bad_answer\": \"This is the worst day ever\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How do you feel about this gift?\",\n",
    "        \"good_answer\": \"I really like this gift\",\n",
    "        \"bad_answer\": \"I dislike this gift\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Do you like croissants?\",\n",
    "        \"good_answer\": \"Croissants are my favorite\",\n",
    "        \"bad_answer\": \"Croissants are disgusting\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How was the movie?\",\n",
    "        \"good_answer\": \"The movie was fantastic\",\n",
    "        \"bad_answer\": \"The movie was terrible\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How's work going?\",\n",
    "        \"good_answer\": \"I got a promotion at work\",\n",
    "        \"bad_answer\": \"I got fired from work\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How was your vacation?\",\n",
    "        \"good_answer\": \"My vacation was amazing\",\n",
    "        \"bad_answer\": \"My vacation was a disaster\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How was the concert?\",\n",
    "        \"good_answer\": \"The concert exceeded my expectations\",\n",
    "        \"bad_answer\": \"The concert was a huge disappointment\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How do you feel about your family?\",\n",
    "        \"good_answer\": \"I'm grateful for my family\",\n",
    "        \"bad_answer\": \"I'm frustrated with my family\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How is the book?\",\n",
    "        \"good_answer\": \"This book is incredibly engaging\",\n",
    "        \"bad_answer\": \"This book is incredibly boring\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How was the restaurant service?\",\n",
    "        \"good_answer\": \"The restaurant service was excellent\",\n",
    "        \"bad_answer\": \"The restaurant service was horrible\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How do you feel about your accomplishments?\",\n",
    "        \"good_answer\": \"I'm proud of my accomplishments\",\n",
    "        \"bad_answer\": \"I'm ashamed of my mistakes\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How is the sunset?\",\n",
    "        \"good_answer\": \"The sunset is breathtakingly beautiful\",\n",
    "        \"bad_answer\": \"The weather is depressingly gloomy\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How did you do on your exam?\",\n",
    "        \"good_answer\": \"I passed my exam with flying colors\",\n",
    "        \"bad_answer\": \"I failed my exam miserably\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How is the coffee?\",\n",
    "        \"good_answer\": \"This coffee tastes perfect\",\n",
    "        \"bad_answer\": \"This coffee tastes awful\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [(input_to_prompt_llama3(example[\"question\"])+example[\"good_answer\"], True) for example in questions_answers]\n",
    "dataset += [\n",
    "    (input_to_prompt_llama3(example[\"question\"]) + example[\"bad_answer\"], False)\n",
    "    for example in questions_answers\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the CAA vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will extract activations from after the '<|start_header_id|>assistant<|end_header_id|>' token\n"
     ]
    }
   ],
   "source": [
    "search_tokens = tokenizer.encode(MODEL_ID_TO_END_OF_INSTRUCTION[MODEL_LLAMA_3_CHAT])[0, 1:]\n",
    "print(\n",
    "    f\"We will extract activations from after the '{tokenizer.decode(search_tokens)}' token\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:01<00:00,  4.55it/s]\n"
     ]
    }
   ],
   "source": [
    "vectors = get_caa_vecs(\n",
    "    labeled_text=dataset,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    layers=range(5, 30),\n",
    "    token_location_fn=after_search_tokens,\n",
    "    search_tokens=search_tokens,\n",
    "    save_to=None,\n",
    "    batch_size=6,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the CAA vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you like cats? I do.\n",
      "I like cats too.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = model.generate_with_steering(\n",
    "    text=[input_to_prompt_llama3(\"Do you like cats?\")],\n",
    "    tokenizer=tokenizer,\n",
    "    steering_configs=[\n",
    "        SteeringConfig(\n",
    "            layer=12,\n",
    "            vector=vectors[12],\n",
    "            scale=2,\n",
    "            steering_fn=add_multiplier,\n",
    "            token_location_fn=all_tokens,\n",
    "        ),\n",
    "    ],\n",
    "    max_n_tokens=30,\n",
    "    save_to=None,\n",
    ")\n",
    "print(results[\"results\"][0][\"output\"].split(\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Do you like cats?<|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Do you like cats? I do.\n",
      "I like cats too.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = model.generate_with_steering(\n",
    "    text=[input_to_prompt_llama3(\"Do you like cats?\")],\n",
    "    tokenizer=tokenizer,\n",
    "    steering_configs=[\n",
    "        SteeringConfig(\n",
    "            layer=12,\n",
    "            vector=vectors[12],\n",
    "            scale=-2,\n",
    "            steering_fn=add_multiplier,\n",
    "            token_location_fn=all_tokens,\n",
    "        ),\n",
    "    ],\n",
    "    max_n_tokens=30,\n",
    "    save_to=None,\n",
    ")\n",
    "print(results[\"results\"][0][\"output\"].split(\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
